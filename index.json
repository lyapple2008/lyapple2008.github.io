[{"content":"WebRTC的语音降噪算法中实现了一个频点维度的语音概率估计器SpeechProbabilityEstimator，本质是一个多特征融合的线性分类器。统计计算以下三种特征，\nLRT Spectral Flatness 谱平坦度 Spectral Difference 谱差 通过tanh将特征变化映射到概率值，使用不同的width参数来调节敏感度，线性加权融合到得最终的语音概率。 接下来完整分析 WebRTC 中用于语音概率估计的三个核心特征（indicator0, indicator1, indicator2）\n1 2 3 4 5 # 代码位置 modules/audio_processing/ns/speech_probability_estimator.h modules/audio_processing/ns/speech_probability_estimator.cc modules/audio_processing/ns/signal_model_estimator.cc modules/audio_processing/ns/signal_model_estimator.h indicator0: Likelihood Ratio Test (LRT) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // Updates the log LRT measures. void UpdateSpectralLrt(rtc::ArrayView\u0026lt;const float, kFftSizeBy2Plus1\u0026gt; prior_snr, rtc::ArrayView\u0026lt;const float, kFftSizeBy2Plus1\u0026gt; post_snr, rtc::ArrayView\u0026lt;float, kFftSizeBy2Plus1\u0026gt; avg_log_lrt, float* lrt) { RTC_DCHECK(lrt); for (size_t i = 0; i \u0026lt; kFftSizeBy2Plus1; ++i) { float tmp1 = 1.f + 2.f * prior_snr[i]; float tmp2 = 2.f * prior_snr[i] / (tmp1 + 0.0001f); float bessel_tmp = (post_snr[i] + 1.f) * tmp2; avg_log_lrt[i] += .5f * (bessel_tmp - LogApproximation(tmp1) - avg_log_lrt[i]); } float log_lrt_time_avg_k_sum = 0.f; for (size_t i = 0; i \u0026lt; kFftSizeBy2Plus1; ++i) { log_lrt_time_avg_k_sum += avg_log_lrt[i]; } *lrt = log_lrt_time_avg_k_sum * kOneByFftSizeBy2Plus1; } LRT衡量的是，当前观察到的频谱，更像是语音信号，还是更像噪声，是在两种假设之间进行比较：\n$H_0$: 当前帧是纯噪声帧 $H_1$: 当前帧是语音 + 噪声帧 通过计算：\n$$ \\Lambda = \\log \\left( \\frac{P(X | H_1)}{P(X | H_0)} \\right) $$\n其中：\n$P(X|H_1)$：给定为语音的条件下，观测值 X 出现的概率； $P(X|H_0)$：给定为噪声的条件下，观测值 X 出现的概率； 最终近似简化为（推导过程跳过了，找了一些资料，没看太懂🙈）：\n$$ \\text{LRT}(k) \\approx \\frac{(1 + \\gamma_k) \\cdot 2 \\cdot \\xi_k}{1 + 2 \\cdot \\xi_k} - \\log(1 + 2 \\cdot \\xi_k) $$\n其中：\n$\\xi_k$：频点 k 的先验 SNR； $\\gamma_k$：频点 k 的后验 SNR； LRT 越大，表明信号更像语音； LRT 越小，说明更像噪声。 再通过tanh函数映射到[0, 1]的区间。\n1 2 3 4 5 6 7 8 9 10 11 // Width parameter in sigmoid map for prior model. constexpr float kWidthPrior0 = 4.f; // Width for pause region: lower range, so increase width in tanh map. constexpr float kWidthPrior1 = 2.f * kWidthPrior0; // Average LRT feature: use larger width in tanh map for pause regions. float width_prior = model.lrt \u0026lt; prior_model.lrt ? kWidthPrior1 : kWidthPrior0; // Compute indicator function: sigmoid map. float indicator0 = 0.5f * (tanh(width_prior * (model.lrt - prior_model.lrt)) + 1.f); indicator1: Spectral Flatness 谱平坦度 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 // Updates the spectral flatness based on the input spectrum. void UpdateSpectralFlatness( rtc::ArrayView\u0026lt;const float, kFftSizeBy2Plus1\u0026gt; signal_spectrum, float signal_spectral_sum, float* spectral_flatness) { RTC_DCHECK(spectral_flatness); // Compute log of ratio of the geometric to arithmetic mean (handle the log(0) // separately). constexpr float kAveraging = 0.3f; float avg_spect_flatness_num = 0.f; for (size_t i = 1; i \u0026lt; kFftSizeBy2Plus1; ++i) { if (signal_spectrum[i] == 0.f) { *spectral_flatness -= kAveraging * (*spectral_flatness); return; } } for (size_t i = 1; i \u0026lt; kFftSizeBy2Plus1; ++i) { avg_spect_flatness_num += LogApproximation(signal_spectrum[i]); } float avg_spect_flatness_denom = signal_spectral_sum - signal_spectrum[0]; avg_spect_flatness_denom = avg_spect_flatness_denom * kOneByFftSizeBy2Plus1; avg_spect_flatness_num = avg_spect_flatness_num * kOneByFftSizeBy2Plus1; float spectral_tmp = ExpApproximation(avg_spect_flatness_num) / avg_spect_flatness_denom; // Time-avg update of spectral flatness feature. *spectral_flatness += kAveraging * (spectral_tmp - *spectral_flatness); } 谱平坦度的定义 $$ \\text{SFM} = \\frac{\\left( \\prod_{i=1}^{N} X_i \\right)^{1/N}}{\\frac{1}{N} \\sum_{i=1}^{N} X_i} = \\frac{\\text{几何均值}}{\\text{算术均值}} $$\n为什么谱平坦度可以区分语音和噪声 首先从纯数学角度，根据均值不等式有：几何均值 \u0026lt;= 算数均值\n当所有元素相等时，两者相等 当元素差异越大时，几何均值相对算数均值越小，说明“越不平坦” 再对应到语音降噪应用：\n噪声（尤其是白噪声）：频带能量均匀，几何均值 ≈ 算数均值，Flatness ≈ 1 语音信号：存在能量集中（共振峰），几何均值 ≪ 算数均值，Flatness 接近 0 因此谱平坦度 = 能量分布的“均匀性量尺” → 能直接用来做语音/噪声分类特征！\n代码实现 计算几何均值 1 2 3 4 for (size_t i = 1; i \u0026lt; kFftSizeBy2Plus1; ++i) { avg_spect_flatness_num += LogApproximation(signal_spectrum[i]); } avg_spect_flatness_num = avg_spect_flatness_num * kOneByFftSizeBy2Plus1; 等价于下面的式子，先算log版本\n$$ \\exp\\left( \\frac{1}{N} \\sum \\log(X_i) \\right) $$\n再做指数运算还原为几何均值\n1 ExpApproximation(avg_spect_flatness_num) 计算算数均值（去掉DC分量） 1 2 3 float avg_spect_flatness_denom = signal_spectral_sum - signal_spectrum[0]; avg_spect_flatness_denom = avg_spect_flatness_denom * kOneByFftSizeBy2Plus1; 得到谱平坦度 1 2 float spectral_tmp = ExpApproximation(avg_spect_flatness_num) / avg_spect_flatness_denom; 平滑更新 1 2 // Time-avg update of spectral flatness feature. *spectral_flatness += kAveraging * (spectral_tmp - *spectral_flatness); indicator2: Spectral Difference 谱模板差异 Spectral Difference频谱差异是用于衡量当前帧的频谱与已学习噪声模板之间的差异程度。其基本思想是：\n如果当前帧的频谱结构与噪声模板相似，则可能是噪声；如果差异大，则可能是语音。\n总体计算公式 $$ \\text{SpectralDiff} = \\text{Var}(X) - \\frac{[\\text{Cov}(X, Y)]^2}{\\text{Var}(Y)} $$ 其中： •\tX：当前帧的 信号频谱； •\tY：历史平均的 噪声频谱（称为 conservative noise spectrum）； •\t$\\mathrm{Var}$：方差（描述“起伏程度”）； •\t$\\mathrm{Cov}$：协方差（描述“是否联动”）。\n为什么它能衡量相似程序？ 从统计角度看，Var(X) - Cov(X, Y)^2 / Var(Y) 是当前帧中 与过去模板不一致的能量。如果：\n如果 signal ≈ noise（噪声帧）：→ covariance² / noise_variance ≈ signal_variance → spectral_diff ≈ 0 如果 signal 包含语音成分（结构和噪声不一样）：→ covariance 小，spectral_diff 增大 这个公式本质上等价于：\nVar(Residual) = Var(Signal) - Var(ProjectedNoiseComponent)\n即：当前帧中除了可以用噪声解释的部分，剩下有多少“异常能量”\n多特征融合 1 2 3 4 // Combine the indicator function with the feature weights. float ind_prior = prior_model.lrt_weighting * indicator0 + prior_model.flatness_weighting * indicator1 + prior_model.difference_weighting * indicator2; 最终组合这三个指标：\n每个特征都有独立的权重 互补性强，提升稳健性 举例子：\n情况 LRT Flatness Spectral Diff 判断结果 短促辅音 [k] 高 高（像噪声） 低（像模板） 不能仅靠 flatness 判断，indicator2 弥补 背景突发噪声 高 高 高 indicator2 抑制误判 语音暂停期 低 高 高 三项均为低，VAD 静音 计算频点的后验语音概率 平滑更新先验语音概率 1 2 3 4 5 // Compute the prior probability. prior_speech_prob_ += 0.1f * (ind_prior - prior_speech_prob_); // Make sure probabilities are within range: keep floor to 0.01. prior_speech_prob_ = std::max(std::min(prior_speech_prob_, 1.f), 0.01f); 计算后验语音概率 因为prior_speech_prob_是通过历史信息估计的当前帧语音概率，因此这个概率称为先验语音概率。实际使用时我们不需要我们观察到当帧后给出的概率，即后验语音概率。 贝叶斯定理给出后验概率公式：\n$$ P(H_1 \\mid X) = \\frac{P(H_1) \\cdot P(X \\mid H_1)}{P(H_1) \\cdot P(X \\mid H_1) + P(H_0) \\cdot P(X \\mid H_0)} $$ 我们引入： •\t$\\text{Prior} = P(H_1)$ •\t$\\text{Gain} = \\frac{1 - \\text{Prior}}{\\text{Prior}}$ •\t$\\text{LRT} = \\frac{P(X \\mid H_1)}{P(X \\mid H_0)}$\n可得后验语音概率（简化推导）：\n$$ P(H_1 \\mid X) = \\frac{1}{1 + \\text{Gain} \\cdot \\frac{1}{\\text{LRT}}} $$\n这正是代码中这段的实现\n1 2 3 4 5 6 7 8 9 // Final speech probability: combine prior model with LR factor:. float gain_prior = (1.f - prior_speech_prob_) / (prior_speech_prob_ + 0.0001f); std::array\u0026lt;float, kFftSizeBy2Plus1\u0026gt; inv_lrt; ExpApproximationSignFlip(model.avg_log_lrt, inv_lrt); for (size_t i = 0; i \u0026lt; kFftSizeBy2Plus1; ++i) { speech_probability_[i] = 1.f / (1.f + gain_prior * inv_lrt[i]); } 其中： •\tgain_prior = (1 - prior_speech_prob_) / (prior_speech_prob_ + ε) •\tinv_lrt[i] = e^{-avg_log_lrt[i]} ≈ 1 / LRT （指数近似）\n利用语音概率辅助更新噪声谱 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 void NoiseEstimator::PostUpdate( rtc::ArrayView\u0026lt;const float\u0026gt; speech_probability, rtc::ArrayView\u0026lt;const float, kFftSizeBy2Plus1\u0026gt; signal_spectrum) { // Time-avg parameter for noise_spectrum update. constexpr float kNoiseUpdate = 0.9f; float gamma = kNoiseUpdate; for (size_t i = 0; i \u0026lt; kFftSizeBy2Plus1; ++i) { const float prob_speech = speech_probability[i]; const float prob_non_speech = 1.f - prob_speech; // Temporary noise update used for speech frames if update value is less // than previous. float noise_update_tmp = gamma * prev_noise_spectrum_[i] + (1.f - gamma) * (prob_non_speech * signal_spectrum[i] + prob_speech * prev_noise_spectrum_[i]); // Time-constant based on speech/noise_spectrum state. float gamma_old = gamma; // Increase gamma for frame likely to be seech. constexpr float kProbRange = .2f; gamma = prob_speech \u0026gt; kProbRange ? .99f : kNoiseUpdate; // Conservative noise_spectrum update. if (prob_speech \u0026lt; kProbRange) { conservative_noise_spectrum_[i] += 0.05f * (signal_spectrum[i] - conservative_noise_spectrum_[i]); } // Noise_spectrum update. if (gamma == gamma_old) { noise_spectrum_[i] = noise_update_tmp; } else { noise_spectrum_[i] = gamma * prev_noise_spectrum_[i] + (1.f - gamma) * (prob_non_speech * signal_spectrum[i] + prob_speech * prev_noise_spectrum_[i]); // Allow for noise_spectrum update downwards: If noise_spectrum update // decreases the noise_spectrum, it is safe, so allow it to happen. noise_spectrum_[i] = std::min(noise_spectrum_[i], noise_update_tmp); } } } 上面这段代码是webrtc中结合语音概率，对之前基于分位数估计得到的噪声谱，进行进一步修正的过程。\n那为什么已经有了基于分位数的噪声估计，还需要在PostUpdate()中进行进一步修正呢？\n如下表，我们对比与初始分位数估计的关系和区别\n特征 PreUpdate() 中的分位数估计 PostUpdate() 中的时间平均更新 原理 统计过往帧的底噪分布（log-domain） 利用当前帧的语音概率进行时间递归更新 更新维度 横向（跨帧分布） 纵向（帧内时间平滑） 响应特性 对背景缓慢变化有响应，对突发语音稳健 在语音帧期间抑制更新，非语音帧中轻微修正 对应变量 quantile_noise_estimator_.Estimate(\u0026hellip;) → noise_spectrum_[] 初值\tnoise_spectrum_[] → 平滑动态追踪修正 目的 建立初步噪声模型 细化并动态追踪噪声谱 总结：为什么需要 PostUpdate？ 分位数估计（PreUpdate）很强健，但慢。 PostUpdate 提供 快速、平滑、概率驱动 的动态调整机制。 防止语音能量污染噪声估计； 保持噪声谱能持续跟踪 非平稳噪声（如空调开关、风声变化）； 为后续 Wiener 滤波器提供更可靠的噪声谱输入。 Ok! 到这里WebRTC就真正完成了噪声谱的估计，接下继续分享WebRTC语音降噪代码.\nTo Be Continue!!! ","permalink":"https://lyapple2008.github.io/posts/2025-06-30-%E8%AF%AD%E9%9F%B3%E6%A6%82%E7%8E%87%E4%BC%B0%E8%AE%A1/","summary":"\u003cp\u003eWebRTC的语音降噪算法中实现了一个频点维度的语音概率估计器SpeechProbabilityEstimator，本质是一个多特征融合的线性分类器。统计计算以下三种特征，\u003c/p\u003e","title":"WebRTC语音降噪之语音概率估计"},{"content":"噪声估计的作用 噪声估计算法在整个语音降噪系统中起到核心支撑作用，先验SNR和后验SNR的计算都依赖于当前帧的噪声功率谱估计。若噪声估计偏低，会导致保留太多噪声（过度保留）；若噪声估计偏高，会把语音当作噪声过滤掉（语音失真）； 更新不稳定，整体听感时好时坏，忽大忽小，出现”泵声“、”音乐噪声“现象。本文介绍WebRTC中目前使用的基于分位数的噪声估计算法，及其在工程实现中的巧妙之处。\n什么是基于分位数的噪声估计 基于分位数的噪声估计算法是一种利用信号统计特性区分噪声和语音的自适应方法。其核心原理在于：噪声的能量分布通常集中在低分位区域，而语音信号的能量分布会抬高高分位数。\nWebRTC中的实现解读（妙呀） 1 2 3 # 代码位置 modules/audio_processing/ns/quantile_noise_estimator.h modules/audio_processing/ns/quantile_noise_estimator.cc 分位数计算 1 2 3 4 5 if (log_spectrum[i] \u0026gt; log_quantile_[j]) { log_quantile_[j] += 0.25f * multiplier; } else { log_quantile_[j] -= 0.75f * multiplier; } 以上是WebRTC中分位数计算的代码，它表示25%分位数估计，下面我们来逐步说明为什么这段代码可以计算25%分位数。\n什么是分位数？ 以25%分位数为例，它表示：如果你观察一组数，有25%是小于它的，有75%是大于它的。 比如：\n1 2 数据（已排序）：[1, 2, 3, 4, 5, 6, 7, 8, 9] 0.25分位数 ≈ 第3个数 = 3 非对称更新的数学直觉 设：当前估计值为Q，当前观测值为X\n我们每帧更新规则如下：\n情况 更新量 含义 x \u0026gt; Q Q ← Q + 0.25 × step 当前值太大，稍微拉高估计值 x \u0026lt; Q Q ← Q - 0.75 × step 当前值太小，大幅拉低估计值 收敛分析：平衡点 = 25%分位数 考虑连续观察大量值 {x₁, x₂, \u0026hellip;, xₙ}，估计值 Q 如果在一个固定位置附近波动，那它一定满足： 平均上调量 ≈ 平均下调量，也就是说，在那个点：\n上调概率 × 上调步长 = 下调概率 × 下调步长\n其中：\n上调概率 p_up = P(x \u0026gt; Q) 下调概率 p_down = P(x \u0026lt; Q) = 1 - p_up 上调步长 = 0.25 下调步长 = 0.75 计算可以得到：p_down = 0.25 ✅ 说明这个估计最终会逼近 25% 分位数！\n这个分位数的实现真是妙了呀，避免了常规分位计算需要排序的问题，同时还可以实时更新。👍\n多分位数估计分时更新 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 // Loop over simultaneous estimates. for (int s = 0, k = 0; s \u0026lt; kSimult; ++s, k += static_cast\u0026lt;int\u0026gt;(kFftSizeBy2Plus1)) { const float one_by_counter_plus_1 = 1.f / (counter_[s] + 1.f); for (int i = 0, j = k; i \u0026lt; static_cast\u0026lt;int\u0026gt;(kFftSizeBy2Plus1); ++i, ++j) { // Update log quantile estimate. const float delta = density_[j] \u0026gt; 1.f ? 40.f / density_[j] : 40.f; const float multiplier = delta * one_by_counter_plus_1; if (log_spectrum[i] \u0026gt; log_quantile_[j]) { log_quantile_[j] += 0.25f * multiplier; } else { log_quantile_[j] -= 0.75f * multiplier; } // Update density estimate. constexpr float kWidth = 0.01f; constexpr float kOneByWidthPlus2 = 1.f / (2.f * kWidth); if (fabs(log_spectrum[i] - log_quantile_[j]) \u0026lt; kWidth) { density_[j] = (counter_[s] * density_[j] + kOneByWidthPlus2) * one_by_counter_plus_1; } } if (counter_[s] \u0026gt;= kLongStartupPhaseBlocks) { counter_[s] = 0; if (num_updates_ \u0026gt;= kLongStartupPhaseBlocks) { quantile_index_to_return = k; } } ++counter_[s]; } kLongStartupPhaseBlocks=200，意味着分位数估计在200帧后，即2秒，才会更新重置输出估计结果。webrtc为了减少响应延迟，设置了三个错位的独立分位数估计器，如下代码，可以看到每一个分位数估计器的更新计数是错开的，这样可以达到每67帧，即670ms，就会有一个分位数估计器进行重置更新输出估计结果，从而达到快速响应的效果。\n1 2 3 4 constexpr float kOneBySimult = 1.f / kSimult; for (size_t i = 0; i \u0026lt; kSimult; ++i) { counter_[i] = floor(kLongStartupPhaseBlocks * (i + 1.f) * kOneBySimult); } 利用”密度“估计实现自适应步长 分位数估计器在更新的时候，其更新步长与这个density_变量直接相关，现在我们来看下webrtc的实现是如何做到自适应步长的。\ndensity_[j]表示: 当前分位数估计点附近的”局部密度估计“，近似表示这个log频谱点的概率密度函数值。\n现实场景中，噪声频点能量分布是变化的，当低噪声变化时，噪声频点能量分布密集；当语音变化时，噪声频点能量分布稀疏。因此需要估计分布密度，以调整步长动态性，防止在高密度或低密度区域过度抖动或者太慢反应\ndensity_是如何计算的 1 2 3 4 5 6 7 // Update density estimate. constexpr float kWidth = 0.01f; constexpr float kOneByWidthPlus2 = 1.f / (2.f * kWidth); if (fabs(log_spectrum[i] - log_quantile_[j]) \u0026lt; kWidth) { density_[j] = (counter_[s] * density_[j] + kOneByWidthPlus2) * one_by_counter_plus_1; } 这是一种滑动窗口统计估计法：\n步骤 说明 fabs(\u0026hellip;) \u0026lt; kWidth 当前观测值是否落在估计值 ±0.01 范围内 kOneByWidthPlus2 = 1 / (2 × 0.01) 这是一个常数权重（经验值） density_[j] = (\u0026hellip;) 使用 指数滑动平均 来更新密度值 最终的效果是： density_[j] 趋近于“单位宽度窗口”内命中次数的平均值 —— 表示在分位点附近的信号频谱密集程度。\n1 2 const float delta = density_[j] \u0026gt; 1.f ? 40.f / density_[j] : 40.f; const float multiplier = delta * one_by_counter_plus_1; density_变量直接影响了分位数估计的步长，也就是说\n如果 density_ 高：说明这个频率点的能量比较“稳定”，变化较小 → delta 会小 → 更新变慢 如果 density_ 低：说明这个点的能量波动大 → delta 会大 → 更新更激进 总结就是，density_ 表示当前分位点附近的局部频谱密度，用于调节更新速率，帮助 WebRTC 实现稳定、鲁棒、快速收敛的底噪估计。\nOk，这就是WebRTC中基于分位数噪声估计的全部了。总的来说，基于分位数的噪声估计算法原理简单，但WebRTC的实现有很多巧妙的地方，即保证了效果，也提高了效率，绝对是工程精华值得好好研究。\n接下来会继续分享WebRTC语音降噪部分代码，希望对有兴趣的朋友有帮助。\n","permalink":"https://lyapple2008.github.io/posts/2025-06-28-%E5%9F%BA%E4%BA%8E%E5%88%86%E4%BD%8D%E6%95%B0%E7%9A%84%E5%99%AA%E5%A3%B0%E4%BC%B0%E8%AE%A1/","summary":"\u003ch2 id=\"噪声估计的作用\"\u003e噪声估计的作用\u003c/h2\u003e\n\u003cp\u003e噪声估计算法在整个语音降噪系统中起到核心支撑作用，先验SNR和后验SNR的计算都依赖于当前帧的噪声功率谱估计。若噪声估计偏低，会导致保留太多噪声（过度保留）；若噪声估计偏高，会把语音当作噪声过滤掉（语音失真）； 更新不稳定，整体听感时好时坏，忽大忽小，出现”泵声“、”音乐噪声“现象。本文介绍WebRTC中目前使用的基于分位数的噪声估计算法，及其在工程实现中的巧妙之处。\u003c/p\u003e","title":"WebRTC语音降噪之基于分位数的噪声估计"},{"content":"程序员的基本修养之代码编译 | 代码编译过程介绍，避坑指南，一些常用代码查看工具使用介绍\n预处理 1.预处理的作用 宏替换：\n替换 #define 定义的宏。\n1 2 3 4 #define PI 3.14159 double circle_area(double radius) { return PI * radius * radius; // 替换后：3.14159 * radius * radius } 头文件包含 替换 #include 指令为头文件的内容。\n1 #include \u0026lt;iostream\u0026gt;// 替换为 \u0026lt;iostream\u0026gt; 文件的完整内容 条件编译 根据条件选择性地编译代码。\n1 2 3 #ifdef DEBUG std::cout \u0026lt;\u0026lt; \u0026#34;Debug mode is on\u0026#34; \u0026lt;\u0026lt; std::endl; #endif 宏展开 处理函数式宏。\n1 2 #define SQUARE(x) ((x) * (x)) int result = SQUARE(5); // 替换为 ((5) * (5)) 注释删除 移除源代码中的注释内容。\n2.查看预处理结果 通过 编译器选项 可以仅执行预处理步骤。例如gcc/clang：\n1 g++ -E main.cpp -o main.i -E 选项表示仅执行预处理。 输出文件 main.i 包含预处理后的源代码。 cmake可以通过添加配置保存中间产物 1 set_target_properties(${PROJECT_NAME} PROPERTIES COMPILE_FLAGS \u0026#34;-save-temps=obj\u0026#34;) 注：一些复杂的宏操作可以通过这种方式确定最终展开后的形式\n3.预处理注意事项 宏展开陷阱 注意宏的嵌套展开可能引发意外行为，用括号保护表达式。\n1 #define ADD(x, y) ((x) + (y)) 头文件滥用 导出了所有头文件并加入了搜索路径，当存在多个同名头文件时，可能会引起一些诡异的编译问题，或者运行时崩溃\n头文件的搜索顺序 1.搜索当前目录（一般是#include \u0026ldquo;header.h\u0026rdquo;，双引号方式引用头文件） 2.通过-I指定的目录，多个目录按加入的顺序搜索 3.标准系统目录 编译 | 编译是从源文件（.c/.cpp）生成目标文件（*.o）的过程\nQ1：目标文件里面包含了哪些信息？ 1 llvm-objdump -s /path/to/objfile # 显示目标文件中所有Section的内容 1.目标文件类型、目标架构 查看命令：\n1 2 # 可以直接使用ndk里面的工具，目标文件/静态库/动态库/可执行文件都可以查看 llvm-readelf -h 目标文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 File: /Users/marshall/Workspace/projects/test_compile/build/lib_src/CMakeFiles/lib_src.dir/conv1d.o Format: Mach-O arm64 Arch: aarch64 AddressSize: 64bit MachHeader { Magic: Magic64 (0xFEEDFACF) CpuType: Arm64 (0x100000C) CpuSubType: CPU_SUBTYPE_ARM64_ALL (0x0) FileType: Relocatable (0x1) NumOfLoadCommands: 4 SizeOfLoadCommands: 520 Flags [ (0x2000) MH_SUBSECTIONS_VIA_SYMBOLS (0x2000) ] Reserved: 0x0 } 2.只读内容存在__TEXT段 a.__text节保存了编译后的机器码: 内容：源代码编译后的二进制机器指令，对应程序的函数和逻辑 查看命令：\n1 llvm-objdump -d 目标文件 注：通过查看中间产物汇编文件（*.s）可以初步分析是否值得做 比如通过查看conv1d.s文件，发现已经做了循环展开，就不需要在c代码上手动做循环展开了（NEON类的SIMD在代码编译是否会进行编译优化待确定）\nb.__cstring节保存了字符串 c.__const节保存了学常量 3.全局变量和静态变量保存在__DATA段 nm命令介绍 nm命令可以用来分析二进制分析的符号信息\n1 2 # -A 选项在符号名前附加文件名，适用于分析静态库（.a）： nm -A 静态库路径 注：该命令可以用来辅助分析Undefined symbol一类的编译问题 Q2：同一份代码，保持编译参数不变的情况，两次编译最终的目标文件是否是一样的？ 通常一致的场景 如果满足以下条件，两次编译的目标文件大概率相同： 代码完全不变：未修改任何源码文件（包括头文件） 编译参数严格一致：包括优化级别（如 -O2）、调试选项（如 -g）、路径参数（如 -I）等 编译器版本一致：同一版本的编译器（如 GCC 12.3）和链接器 环境无干扰：\na.无时间戳或随机化因素嵌入二进制文件（如代码中未使用 DATE、TIME 宏）\nb.编译路径和文件系统结构相同 可能导致不一致的例外情况 时间戳或随机化因素 若源码使用 DATE、TIME 等宏，编译后生成的二进制文件会包含编译时间戳，导致两次编译结果不同。 1 printf(\u0026#34;Build Time: %s %s\\n\u0026#34;, __DATE__, __TIME__); // 每次编译结果不同 调试信息中的路径差异 调试信息（.debug_line 段）默认包含源码绝对路径。若两次编译的源码目录不同，目标文件会不同。 1 2 3 4 5 # 第一次编译路径：/home/user/project/ gcc -g main.c -o main # 第二次编译路径：/tmp/build/ gcc -g main.c -o main # 调试信息中的路径不同，目标文件哈希值不同 链接 | 链接就是把所有目标文件合并到起，同时目标文件中在未知的地址（如在其它文件中实现的函数调用）替换成最终的地址\n注：左侧是目标文件main.o，右侧是最终的可执行程序main ``` Shell llvm-objdump -d 目标文件 ``` 动态库与静态库对比 macos环境下查看依赖\n1 otool -L /path/to/binary 1 2 3 main: @rpath/liblib_src.dylib (compatibility version 0.0.0, current version 0.0.0) /usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1351.0.0) 注：@rpath 会根据不同应用的配置解析到对应的目录\n","permalink":"https://lyapple2008.github.io/posts/2025-03-27-%E4%BB%A3%E7%A0%81%E7%BC%96%E8%AF%91/","summary":"\u003ch1 id=\"程序员的基本修养之代码编译\"\u003e程序员的基本修养之代码编译\u003c/h1\u003e\n\u003cp\u003e| 代码编译过程介绍，避坑指南，一些常用代码查看工具使用介绍\u003c/p\u003e\n\u003ch2 id=\"预处理\"\u003e预处理\u003c/h2\u003e\n\u003ch3 id=\"1预处理的作用\"\u003e1.预处理的作用\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e宏替换\u003c/strong\u003e：\u003cbr\u003e\n替换 #define 定义的宏。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e4\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-C\" data-lang=\"C\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cp\"\u003e#define PI 3.14159\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cp\"\u003e\u003c/span\u003e\u003cspan class=\"kt\"\u003edouble\u003c/span\u003e \u003cspan class=\"nf\"\u003ecircle_area\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003edouble\u003c/span\u003e \u003cspan class=\"n\"\u003eradius\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003ePI\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003eradius\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003eradius\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e \u003cspan class=\"c1\"\u003e// 替换后：3.14159 * radius * radius\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e\u003cstrong\u003e头文件包含\u003c/strong\u003e \u003cbr\u003e\n替换 #include 指令为头文件的内容。\u003c/p\u003e","title":"程序员的基本修养之代码编译"},{"content":" 本文记录在mac mini m2 pro上搭建tensorflow gpu版本的过程，中间因为一些坑，导致环境搭建过程有点反复，希望对有相同需求的同学有帮助。 1. 安装conda环境 这里创建conda环境的时候需要指定python版本为3.9，这是因为后面需要安装的tensorflow需要python 3.6~3.9\n1 conda create -n tf_gpu python=3.9 2. 安装tensorflow 安装tensorflow的时候，需要指定版本为2.14.1，这里是因为后面需要安装的tensorflow-metal，最新版本只支持2.14\n1 pip install tensorflow==2.14.1 3. 安装tensorflow-metal 目前tensorflow官方没有支持apple gpu，需要通过tensorflow-metal插件来支持使用apple gpu\n1 pip install tensorflow-metal==1.1.0 4. 测试验证 1 2 3 4 5 import tensorflow.keras import tensorflow as tf print(f\u0026#34;Tensor Flow Version: {tf.__version__}\u0026#34;) gpu = len(tf.config.list_physical_devices(\u0026#39;GPU\u0026#39;))\u0026gt;0 print(\u0026#34;GPU is\u0026#34;, \u0026#34;available\u0026#34; if gpu else \u0026#34;NOT AVAILABLE\u0026#34;) 成功后会得到下面的输出\n1 2 3 4 Tensor Flow Version: 2.14.1 Scikit-Learn 1.5.0 SciPy 1.13.1 GPU is available ","permalink":"https://lyapple2008.github.io/posts/2024-06-10-macos-m2-tensorflow-gpu%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/","summary":"\u003c!-- {% asset_img title.gif %} --\u003e\n\u003cimg src=\"/images/2024-06-10-title.gif\"/\u003e\n本文记录在mac mini m2 pro上搭建tensorflow gpu版本的过程，中间因为一些坑，导致环境搭建过程有点反复，希望对有相同需求的同学有帮助。","title":"MacOS M2 Tensorflow GPU环境搭建"},{"content":"什么是算子融合 算子融合是一种通过合并计算图中多个算子到一个算子，达到减少计算量和内存访问的优化方法。\nConv + BatchNormalization + ReLu融合 从融合后的最终公式可以看出，可以在初始阶段就通过BN的均值和方差（推理阶段BN的均值和方差是常量）更新Conv层的weights和bias参数，这样融合后的算子相当少了一个BN层的操作，既减少了内存访问，也减少了计算量\n如何做算子融合 算子分类 当存在多个输入，同时存在多种输入-输出映射关系时，最终的Mapping type由最复杂的那一个决定。Mapping type复杂度递增顺序：One-to-One，Reorganize，Shuffle，One-to-Many，Many-to-Many 注：Many-to-Many包含Many-to-One的类型\n算子可融合性 绿色：可以融合，且有收益 黄色：要做profile才能确定是否有收益 红色：没有收益，不融合\n计算图基于融合性分块 分块过程：\n随机挑选一个One-to-One 算子节点做为种子节点 从种子节点往后进行融合，直到没有可以融合的节点，并更新块的Mapping Type 从种子节点往前进行融合，直到没有可以融合的节点，并更新块的Mapping Type 重复执行1、2、3，直到没有可用的种子节点 融合代码生成 基于编译生成融合代码（DNNFusion、TVM）\n参考\nDNNFusion: accelerating deep neural networks execution with advanced operator fusion ","permalink":"https://lyapple2008.github.io/posts/2024-05-17-%E7%AE%97%E5%AD%90%E8%9E%8D%E5%90%88/","summary":"\u003ch2 id=\"什么是算子融合\"\u003e什么是算子融合\u003c/h2\u003e\n\u003cp\u003e算子融合是一种通过合并计算图中多个算子到一个算子，达到减少计算量和内存访问的优化方法。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConv + BatchNormalization + ReLu融合\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- {% asset_img conv_bn_relu.png %} --\u003e\n\u003cimg src=\"/images/2024-05-17-conv_bn_relu.png\"/\u003e\n\u003cp\u003e从融合后的最终公式可以看出，可以在初始阶段就通过BN的均值和方差（推理阶段BN的均值和方差是常量）更新Conv层的weights和bias参数，这样融合后的算子相当少了一个BN层的操作，既减少了内存访问，也减少了计算量\u003c/p\u003e","title":"算子融合"},{"content":"正在研究的项目 《人人都用能英语》 看了后发现之前关于“学”英语很多的观念都是错了\n有意思博文记录 TinyProject 通过记录完成一个小项目的方式来进行学习\u0026hellip;\n思考 2022.07.20 今天乐乐出生了，从些人生多一个父亲的身份，很欢喜，也很彷徨。\n","permalink":"https://lyapple2008.github.io/about_me/","summary":"\u003ch3 id=\"正在研究的项目\"\u003e正在研究的项目\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://xiaolai.co/books/c558c667ad9f05ddce38f06df2d15aa3/chapter1.html\"\u003e《人人都用能英语》\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003e看了后发现之前关于“学”英语很多的观念都是错了\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"有意思博文记录\"\u003e有意思博文记录\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://tinyprojects.dev/\"\u003eTinyProject\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003e通过记录完成一个小项目的方式来进行学习\u0026hellip;\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"思考\"\u003e思考\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e2022.07.20\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e今天乐乐出生了，从些人生多一个父亲的身份，很欢喜，也很彷徨。\u003c/p\u003e","title":"关于我"},{"content":"Android中几种sdkVersion的区别 最近遇到一个由于升级了targetSdkVersion而引起的线上crash，之前一直对于Android里面几个sdkVersion的含义和作用很模糊，正好这次把这几个不同的sdkVersion理清楚。\nminSdkVersion 用于指定应用运行所需最低API级别的整数。如果系统的API级别低于属性中指定的值，Android系统将阻止用户安装应用。\ncompileSdkVersion compileSdkVersion只是用来告诉Gradle用哪个Android SDK版本编译你的应用，当使用到新添加的API时就需要使用对应Level的Android SDK。这里需要强调的是，compileSdkVersion只会影响编译的时候，例如，当前我们想使用Android 12一个新的API功能，这时我们就需要将compileSdkVersion升级到31\n1 2 3 4 android { compileSdkVersion 31 ... } 但是这里我们指定了compileSdkVersion到新版本只是让APP编译可以通过，因为在旧的Android系统上还没有使用的新的API，因此在实际代码中还需要对运行时的系统API级别进行判断，保证使用的新API只会在Android12以上的系统运行。\ntargetSdkVersion 要理解targetSdkVersion，需要知道targetSdkVersion在Android中的作用。targetSdkVersion是Android系统提供向后兼容的主要手段（即：新版本SDK手机兼容旧版本SDK工程）。这是什么意思呢？随着 Android 系统的升级，某个系统的 API 或者模块的行为可能会发生改变，但是为了保证老 APK 的行为还是和以前兼容。只要 APK 的 targetSdkVersion 不变，即使这个 APK 安装在新 Android 系统上，其行为还是保持老的系统上的行为，这样就保证了系统对老应用的前向兼容性。 总结： android更新api大概有两种，一种是完全重写（这种就不干targetSdk什么事了）；另一种，保留了老版本的处理逻辑，同时又新增了新的逻辑（用if else的方式来判断具体运行哪段逻辑）。而targetSdk就是用来判断这个if-else的。\n","permalink":"https://lyapple2008.github.io/posts/2022-08-24-android%E4%B8%AD%E5%87%A0%E7%A7%8Dsdkversion%E7%9A%84%E5%8C%BA%E5%88%AB/","summary":"\u003ch1 id=\"android中几种sdkversion的区别\"\u003eAndroid中几种sdkVersion的区别\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e最近遇到一个由于升级了targetSdkVersion而引起的线上crash，之前一直对于Android里面几个sdkVersion的含义和作用很模糊，正好这次把这几个不同的sdkVersion理清楚。\u003c/p\u003e\u003c/blockquote\u003e\n\u003c!-- {% asset_img 001.png %} --\u003e\n\u003cimg src=\"/images/2022-08-24-001.png\"/\u003e","title":"Android中几种sdkVersion的区别"},{"content":"分析综合滤波器组的作用 由于音频信号在不同的频率范围具有不同的特性，因此在音频处理之前通常都会使用分析综合滤波器组将音频信号分成不同的频率子带，再分别进行处理。比如，音频编码中常用到的子带编码（Subband coding）；webrtc的VAD中，会对不同的频率子带计算特征，再综合这些特征计算语音概率。\n分析综合滤波器组是如何实现 在WebRTC中使用最多的是基于IIR实现的二通道QMF分析综合滤波器组，通过二通道QMF滤波器组，可以很方便实现N等分的子带分解，因此这里只介绍二通道QMF滤波器组。\n从上图可以看出，分析综合滤波器包含分析部分和综合部分，当中间没有任何处理时，整个系统的输入输出关系如下：\n可以看到分析滤波器部分的高通和低通部分的频率响应网线正好是相对pi/2镜像对称的，QMF叫镜像滤波器的由来。图中Xa0代表的就是输入信号的低频部分，而Xa1代表的就是输入信号的高频部分，这样通过分析滤波器后，就可以对信号的低频部分和高频部分进行分别处理了。\n因此只要按下面的等式进行滤波器设计，就可以让A(z)=0，即消除混叠，实现完善重构。 为了效率，通常会采用多相形式实现QMF组，如下图所示，信号处理前都会进行抽取操作，这些实际处理的数据量就减少了，从而提升了执行效率\n在QMF组的多相形式中对应的低通滤波器和高通滤波器如上式所示。 WebRTC中的实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 void WebRtcSpl_AnalysisQMF(const int16_t* in_data, size_t in_data_length, int16_t* low_band, int16_t* high_band, int32_t* filter_state1, int32_t* filter_state2) { size_t i; int16_t k; int32_t tmp; int32_t half_in1[kMaxBandFrameLength]; int32_t half_in2[kMaxBandFrameLength]; int32_t filter1[kMaxBandFrameLength]; int32_t filter2[kMaxBandFrameLength]; const size_t band_length = in_data_length / 2; RTC_DCHECK_EQ(0, in_data_length % 2); RTC_DCHECK_LE(band_length, kMaxBandFrameLength); // Split even and odd samples. Also shift them to Q10. for (i = 0, k = 0; i \u0026lt; band_length; i++, k += 2) { half_in2[i] = ((int32_t)in_data[k]) * (1 \u0026lt;\u0026lt; 10); half_in1[i] = ((int32_t)in_data[k + 1]) * (1 \u0026lt;\u0026lt; 10); } // All pass filter even and odd samples, independently. WebRtcSpl_AllPassQMF(half_in1, band_length, filter1, WebRtcSpl_kAllPassFilter1, filter_state1); WebRtcSpl_AllPassQMF(half_in2, band_length, filter2, WebRtcSpl_kAllPassFilter2, filter_state2); // Take the sum and difference of filtered version of odd and even // branches to get upper \u0026amp; lower band. for (i = 0; i \u0026lt; band_length; i++) { tmp = (filter1[i] + filter2[i] + 1024) \u0026gt;\u0026gt; 11; low_band[i] = WebRtcSpl_SatW32ToW16(tmp); tmp = (filter1[i] - filter2[i] + 1024) \u0026gt;\u0026gt; 11; high_band[i] = WebRtcSpl_SatW32ToW16(tmp); } } 上面是WebRTC中关于分析滤波器部分的实现，从代码中可以看出WebRTC中的分析综合滤波器是基于全通滤波器的QMF多相实现，其中的全通滤器采用了IIR实现，即其中的P0(z)和P1(z)都是全通滤波器。通过参考[2]我们可以梳理这个问题的处理流程，通过分析QMF分析综合滤波器满足完美重构的条件，可以得到H0、H1、G0、G1之间的关系，同时H0和H1是基于pi/2，因此只需要知道H0，最终转化成H0低通滤波器的设计问题。进一步的由于采用了基于IIR的全通滤波器，因此只需要考虑相位失真问题，最终QMF分析综合滤波器问题转换成了滤波器的相位均衡问题。虽然我们知道了设计QMF分析综合滤波器的原理和思路，但是想设计一个完全可用的滤波器还是很有难度的，下面我们直接看下WebRTC中QMF分析综合滤波器的效果，如下图所示，可以看到对应的低通滤波器和高通滤波器都有很窄的过渡带，整个系统的幅值响应几乎接近0dB的，同时除了pi/2附近的频带都是近似线性相位的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 import scipy.signal as signal import numpy as np import matplotlib.pyplot as plt import control def analysis_synthesis_filter(): filter1_coef = [6418.0 / 65536.0, 36982.0 / 65536.0, 57261.0 / 65536.0] filter2_coef = [21333.0 / 65536.0, 49062.0 / 65536.0, 63010.0 / 65536.0] ha0_0_b = [1, 0, filter1_coef[0]] ha0_0_a = [filter1_coef[0], 0, 1] ha0_1_b = [1, 0, filter1_coef[1]] ha0_1_a = [filter1_coef[1], 0, 1] ha0_2_b = [1, 0, filter1_coef[2]] ha0_2_a = [filter1_coef[2], 0, 1] ha1_0_b = [1, 0, filter2_coef[0]] ha1_0_a = [filter2_coef[0], 0, 1] ha1_1_b = [1, 0, filter2_coef[1]] ha1_1_a = [filter2_coef[1], 0, 1] ha1_2_b = [1, 0, filter2_coef[2]] ha1_2_a = [filter2_coef[2], 0, 1] ha0_b = np.convolve(ha0_2_b, np.convolve(ha0_1_b, ha0_0_b)) ha0_a = np.convolve(ha0_2_a, np.convolve(ha0_1_a, ha0_0_a)) ha1_b = np.convolve([1, 0], np.convolve(ha1_2_b, np.convolve(ha1_1_b, ha1_0_b))) ha1_a = np.convolve(ha1_2_a, np.convolve(ha1_1_a, ha1_0_a)) ha0_sys = control.TransferFunction(ha0_b, ha0_a) ha1_sys = control.TransferFunction(ha1_b, ha1_a) b0_sys = ha1_sys b1_sys = ha0_sys delay_sys = control.TransferFunction([1, 0], [1]) lowpass_sys = 0.5 * (ha0_sys + ha1_sys) highpass_sys = 0.5 * (ha0_sys - ha1_sys) t_sys = delay_sys / 2 * (ha0_sys * b0_sys + ha1_sys * b1_sys) # print(t_sys) lowpass_num = lowpass_sys.num[0][0] lowpass_den = lowpass_sys.den[0][0] highpass_num = highpass_sys.num[0][0] highpass_den = highpass_sys.den[0][0] t_num = t_sys.num[0][0] t_den = t_sys.den[0][0] lowpass_w, lowpass_h = signal.freqz(lowpass_num, lowpass_den) highpass_w, highpass_h = signal.freqz(highpass_num, highpass_den) t_w, t_h = signal.freqz(t_num, t_den) fig, axes = plt.subplots(3, 2) axes[0, 0].plot(lowpass_w/np.pi, 20*np.log10(np.abs(lowpass_h))) axes[0, 1].plot(lowpass_w/np.pi, np.unwrap(np.angle(lowpass_h, deg=True))) axes[1, 0].plot(highpass_w/np.pi, 20*np.log10(np.abs(highpass_h))) axes[1, 1].plot(highpass_w/np.pi, np.unwrap(np.angle(highpass_h, deg=True))) axes[2, 0].plot(t_w/np.pi, 20*np.log10(np.abs(t_h))) axes[2, 1].plot(t_w/np.pi, np.unwrap(np.angle(t_h, deg=True))) plt.show() 最近的一些心得 最后说下近期的两点小心得：\n不懂的知识点请尽早弄懂它。其实对于QMF分析综合滤波器组，在最开始学习音频编码时就遇到了，但是当时没有深入的去搞清楚，最后还是没有躲过去。所以最近也是恶补了很多基础知识，才大致了解了QMF的设计思路。 不断地输出也许是应对焦虑的一种方法。人到中年难免焦虑，就不停地去学习去吸收，但是往往又是很低效的，时间花了，却什么也没有留下。特别是现在这个信息爆炸的时代，各种信息流，碎片化阅读，让我们看起来收获了很多，其实什么也没有。其实我们要对自己是几平米的房子有基本的认知，小房子就应该放少的、小的东西，定期对房间进行整理，永远保持一定的空间的留白，才会有喘息和美的余地。 参考 Book:《数字信号处理：理论、算法与应用》 Paper: 具有良好重建特性的正交镜像IIR滤波器组的设计新方法 Paper: IIR QMF-bank design for speech and audio subband coding Blog: WebRTC VAD 中所用滤波器之分析_book_bbyuan的博客-CSDN博客 ","permalink":"https://lyapple2008.github.io/posts/2022-02-03-qmf%E5%88%86%E6%9E%90%E7%BB%BC%E5%90%88%E6%BB%A4%E6%B3%A2%E5%99%A8/","summary":"\u003ch1 id=\"分析综合滤波器组的作用\"\u003e分析综合滤波器组的作用\u003c/h1\u003e\n\u003cp\u003e由于音频信号在不同的频率范围具有不同的特性，因此在音频处理之前通常都会使用分析综合滤波器组将音频信号分成不同的频率子带，再分别进行处理。比如，音频编码中常用到的子带编码（Subband coding）；webrtc的VAD中，会对不同的频率子带计算特征，再综合这些特征计算语音概率。\u003c/p\u003e\n\u003c!-- {% asset_img 001.png %} --\u003e\n\u003cimg src=\"/images/2022-02-03-001.png\"/\u003e","title":"QMF分析综合滤波器"},{"content":" WebRTC由音频引擎、视频引擎和传输模块，音频处理在WebRTC占了很大一块，本文主要对WebRTC中涉及的音频处理进行简单介绍，不会对具体的实现进行介绍。上图是WebRTC中音频处理的流水线（上面的图是网上的图，如有侵权，通知即删），可以看出主要包含了音频采集播放、音频处理、音频编解码和音频传输。下面对这些模块逐一进行介绍。\n音频采集播放 自然界的声音，包括人说出来的声音，都是模拟信号，这些模拟信号是不能被计算机存储和识别的，也不能通过网络进行传输。音频采集就是声音从模拟信号转换成数字信号的过程，而音频播放就是声音从数字信号转换成模拟信号的过程。对于音频数字信号来说有以下几个最主要的参数：\n采样率 采样率是指录音设备在一秒钟内对声音信号的采样次数，单位是Hz，采样频率越高，声音的还原度越真实越自然。但是人耳可以听到的频度范围是20Hz~20000Hz，然后根据采样定理，也就是说最低只需要40kHz采样率就可以满足人耳的需求了，因此CD音质和音乐音频都是44.1kHz。但是采样率越高意味着数据量越大，因此在一些特殊的场景会使用更低的采样率，如语音通话场景，因为人声都是低于4kHz，因此在语音通话场景更多会使用8kHz或者16kHz的采样率。 采样位宽 声音模拟信号经过采样后得一个个样点的值，这个值需要存储到计算机中，那么使用多少位数来表示这个 就是采样位宽，通常使用最多的是16bit，正好就是一个short类型。 声道数 在使用录音设备进行声音采集时，只能表示到采集点处的声音信号，但是实际声音是有空间信息，为了表达声音的空间信息，就有了声道的概念。这里最有名的可能就是杜比全景声了，大家在电影院看电影的时候，感受到的被声音包围的真实感就是多声道的功劳了，当然这里并不是加多几个通道这么简单啦。 音频编解码 虽然音频的数据量没有视频的量那么大，但是如果直接传输原始的音频数据（原始的音频数据有个名字叫Pulse Code Modulation (PCM)），数据量也是挺大的。现在我们来计算下1秒采样率为48kHz双通道的音频信号的数据量有多大，1秒 * 48000个采样点 * 2个声道 * 每个样点2Bytes，这样1秒的数据量是192kB字节。这样的数据量在互联网发展的早期也是不可以接受的，这时候就需要音频编解码。简单的说，音频编解码就是利用人耳的心理声学特性将也一些不影响听觉的信号丢弃，从而减少信号量的方法，在WebRTC中用得最多的是Opus格式。对于音频编码器来说一个很重要的参数是码率，单位是kbps，即每秒的音频使用多少bits位来表示。音频编解码是音频领域一个很专业的领域，这里就不展开了。\n音频处理 在实时通话场景，需要面对各种复杂的环境，处理各种音频问题，最常见的如噪声、回声、声音过大过小等，WebRTC中有一个专门的音频处理模块来处理这些问题，下面就简单介绍下这些模块。\n回声消除（Acoustic Echo Cancellation） 实际通话场景是一个全双工通信系统非容易产生回声，如下图所示，远端说话声==》近端扬声器播放==》近端麦克风录制==》通过网络传输到远端的扬声器播放，经过这样一个音频环路后，远端又在扬声器里听到自己的声音，也就是回声，如果回声的延时很低时还会产生啸叫，这样是很影响通话体验的。这时就需要回声消除AEC模块了，通常录音数据都会先经过个模块，在传输前先把远端播放的数据消除，这样远端在播放的时候就不会听到回声了。回声消除AEC详细的原理介绍留到后面再介绍，这里先挖个坑。 噪声抑制（Noise Suppression） 噪声抑制NS这个很好理解，实际的通话场景都会存在各种各样的噪声，为了保证通话体验不受影响，这时就需要噪声抑制NS模块了。这里也再挖一坑，后面再根据WebRTC源码进行噪声抑制原理介绍。 自动增益控制（Auto Gain Control） 在实际通话过程中，由于使用设备的差异和通话时离麦克风的远近，导致了通话的音量差异，为了达到统一的体验就需要进行自动增益控制，简单说就是当音量小时调大增益，当音量大时调小增益，达到减少音量起伏的作用。 混音（Mix） 在多人通话场景下，我们需要接收和播放的通常不止一条音频流，但通常只有一个播放设备，因此通常需要对多条音频流先进行混音操作，再进行播放。 音频传输 目前WebRTC音频传输是UDP/RTP/RTCP协议基础上进行传输的，底层UDP协议的不可靠性，导致丢包不可避免，同时音频数据与其它的数据内容有其特殊性，通常一点音频异常都很容易被人感知出来。因此WebRTC针对音频传输做了很多额外的工作，除了最常见的丢包重传，丢包补偿等，还有一个NetEQ模块，会在播放端进行音频播放的加减速来进一步减少由于网络抖动引起的音频异常。\n到这里WebRTC中跟音频相关的技术模块都简单的过了一遍，这样大家对于WebRTC音频处理有个大概的印象，这里的每个模块值得深入去学习，后面也把自己学习的一些心得记录在这里，大家一起学习呀。\n","permalink":"https://lyapple2008.github.io/posts/2021-12-22-webrtc%E4%B8%AD%E7%9A%84%E9%9F%B3%E9%A2%91%E5%A4%84%E7%90%86%E6%A6%82%E8%A7%88/","summary":"\u003c!-- {% asset_img 001.png %} --\u003e\n\u003cimg src=\"/images/2021-12-22-001.png\"/\u003e\n\u003cp\u003eWebRTC由音频引擎、视频引擎和传输模块，音频处理在WebRTC占了很大一块，本文主要对WebRTC中涉及的音频处理进行简单介绍，不会对具体的实现进行介绍。上图是WebRTC中音频处理的流水线（上面的图是网上的图，如有侵权，通知即删），可以看出主要包含了音频采集播放、音频处理、音频编解码和音频传输。下面对这些模块逐一进行介绍。\u003c/p\u003e","title":"WebRTC中的音频处理概览"},{"content":"WebRTC Android源码编译 对于WebRTC的学习来说，首先需要搞定的是源码的编译，由于国内的特殊环境和WebRTC本身的复杂性，导致WebRTC源码编译成了WebRTC学习的第一道门槛。这里把自己在编译WebRTC源码过程中遇到的一些坑进行总结分享，希望对大家有帮助。主要参考官网和WebRTC的编译配置脚本，整个操作过程需要具备科学上学的环境，第一次下载好环境和源码后，后面就不需要了。这里以Android端为例，编译环境为Ubuntu 18.04（再高版本会提示不支持），WebRTC官网有提到Android端暂时只支持在Linux下编译。虽然网上也有资源介绍在Mac环境下编译的，但是用虚拟机装个Ubuntu还是挺方便的，咱就不折腾了。\n安装depots_tools工具包 1 git clone https://chromium.googlesource.com/chromium/tools/depot_tools.git 将depot_tools目录添加到环境变量PATH中\n1 2 3 vim ~/.profile export PATH=\u0026#34;$PATH:/path/to/depot_tools\u0026#34; source ~/.profile 下载WebRTC源码 1 fetch --nohooks webrtc_android 由于WebRTC源码比较大，中间网络问题可能会出错，出错的时候用gclient sync继续就可以了\n下载编译所需依赖包和工具 1 2 ./build/install-build-deps.sh --no-chromeos-fonts . /build/install-build-deps-android.sh 下载指定分支 1 2 $ git checkout -b my_branch refs/remotes/branch-heads/83 $ gclient sync 源码编译 第一条命令是生成编译工程的，可以添加一些参数来控制编译生成，这里可以研究下源码里的webrtc.gni文件，里面有可以指定的参数，这里主要介绍三个：\ntarget_os：因为是在Android平台上运行的，因此这里指定为\u0026quot;android\u0026quot; target_cpu: 这里指定运行的硬件平台，arm平台则是\u0026quot;arm\u0026quot;，如果是arm64平台则是\u0026quot;arm64\u0026quot; is_debug：表示生成是否是debug包\n第二条命令是启动ninja开始编译，编译成功后，会在out_arm/debug目录生成对应的jar包和so库文件\n1 2 gn gen out_arm/debug --args=\u0026#39;is_debug=true target_os=\u0026#34;android\u0026#34; target_cpu=\u0026#34;arm\u0026#34; rtc_include_tests=false rtc_build_tools=false rtc_build_examples=false\u0026#39; ninja -C out_arm/debug 编译问题解决 问题一：gn.py运行失败 gn.py: Could not find checkout in any parent of the current path. This must be run inside a checkout.\n这个问题通常发生在，移动了WebRTC源码目录的时候。这里需要看下是否已经把下载的WebRTC源码都完整拷贝了，进到下载WebRTC源码的目录可以看到这个目录还有几个隐藏目录和文件，这几个文件也是需要一起拷贝过去的，后面用gn命令生成编译工程的时候会去检查这几个文件。\n.cipd .gclient .gclient_entries\n问题二： chromium style问题 clang.gni中关闭chromium style检查，这里只是暂时关闭，为了代码风格的统一介绍还是按照chromium style还添加自己的代码\n1 2 3 4 5 6 7 declare_args() { # Indicates if the build should use the Chrome-specific plugins for enforcing # coding guidelines, etc. Only used when compiling with Clang. clang_use_chrome_plugins = false # is_clang \u0026amp;\u0026amp; !is_nacl \u0026amp;\u0026amp; !use_xcode_clang clang_base_path = default_clang_base_path } 参考：\nWebRTC官网 ","permalink":"https://lyapple2008.github.io/posts/2020-09-07-webrtc-android%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/","summary":"\u003ch1 id=\"webrtc-android源码编译\"\u003eWebRTC Android源码编译\u003c/h1\u003e\n\u003cp\u003e对于WebRTC的学习来说，首先需要搞定的是源码的编译，由于国内的特殊环境和WebRTC本身的复杂性，导致WebRTC源码编译成了WebRTC学习的第一道门槛。这里把自己在编译WebRTC源码过程中遇到的一些坑进行总结分享，希望对大家有帮助。主要参考官网和WebRTC的编译配置脚本，整个操作过程需要具备科学上学的环境，第一次下载好环境和源码后，后面就不需要了。这里以Android端为例，编译环境为Ubuntu 18.04（再高版本会提示不支持），WebRTC官网有提到Android端暂时只支持在Linux下编译。虽然网上也有资源介绍在Mac环境下编译的，但是用虚拟机装个Ubuntu还是挺方便的，咱就不折腾了。\u003c/p\u003e","title":"WebRTC系列-WebRTC_Android源码编译"},{"content":"问题描述 原来的github.io自定义域名博客不能访问，提示如下信息\n1 2 Fastly error: unknown domain: beyoung.xyz. Please check that this domain has been added to a service. Details: cache-lax8629-LAX 问题原因 Github Pages修改了公布的IP，可以到这个网址查到Github Pages目前公布的最新IP\nhttps://help.github.com/en/github/working-with-github-pages/managing-a-custom-domain-for-your-github-pages-site\n解决方案 只需在将原来在阿里云上配置的CNAME，修改到最新的Github Pages上\n在Ping中检验是否已经修改到最新的IP上，如果已经生效，则原来的博客就可以访问了\n","permalink":"https://lyapple2008.github.io/posts/2020-04-25-github-io%E5%8D%9A%E5%AE%A2%E6%97%A0%E6%B3%95%E8%AE%BF%E9%97%AE%E9%97%AE%E9%A2%98/","summary":"\u003ch3 id=\"问题描述\"\u003e问题描述\u003c/h3\u003e\n\u003cp\u003e原来的github.io自定义域名博客不能访问，提示如下信息\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e2\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eFastly error: unknown domain: beyoung.xyz. Please check that this domain has been added to a service.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eDetails: cache-lax8629-LAX\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e","title":"github.io博客无法访问问题"},{"content":"\n为什么需要代码优化 提升体验，扩展玩法 减少限制跟要求，降低门槛 ，覆盖更多群体 在谷歌内部，打造AI应用有两条思路，一是让更多人受惠，二是尽可能减少限制和要求\n场景限制必须进行优化 个人隐私越来越被重视，以往的云端处理方案存在局限性 一些场景要求算法要有极低的延时，如实时通信项目\nFlag终极目标：让算法随手可得 传统代码优化 rnnoise优化举例\n函数近似和查表优化sigmoid函数计算 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 static OPUS_INLINE float tansig_approx(float x) { int i; float y, dy; float sign=1; /* Tests are reversed to catch NaNs */ if (!(x\u0026lt;8)) return 1; if (!(x\u0026gt;-8)) return -1; #ifndef FIXED_POINT /* Another check in case of -ffast-math */ if (celt_isnan(x)) return 0; #endif if (x\u0026lt;0) { x=-x; sign=-1; } i = (int)floor(.5f+25*x); x -= .04f*i; y = tansig_table[i]; dy = 1-y*y; y = y + x*dy*(1 - y*x); return sign*y; } static OPUS_INLINE float sigmoid_approx(float x) { return .5 + .5*tansig_approx(.5*x); } 最粗暴的函数近似方法：\n``` atan(pi*x/2)*2/pi 24.1 ns atan(x) 23.0 ns 1/(1+exp(-x)) 20.4 ns 1/sqrt(1+x^2) 13.4 ns erf(sqrt(pi)*x/2) 6.7 ns tanh(x) 5.5 ns x/(1+|x|) 5.5 ns ``` 减少程序跳转优化RNN计算 CPU多级Cache机制 深度学习移动端优化 网络剪枝 网络的参数都存在冗余的，所以可以进行网络剪枝 网络剪枝的流程 权值的重要性：计算L1或者L2\n神经元的重要性：不为零的次数 剪权值VS剪神经元 剪权值 剪神经元\n剪权值：模型不规则，不便于实现和加速\n剪神经元：模型规则，便于实现和加速\n精简模型设计 标准CNN 深度分离卷积Depthwise Separable Convolution 【参见MobileNet】 参数比较 四、深度学习移动端部署工具 与PC端深度学习环境被大厂垄断不同，移动端的部署工具可以说是百家争鸣，很多深度学习的厂商都会推出自家的推理加速工具。 4.1 硬件厂商 公司 硬件架构 开发工具 海思 NPU HiAI Foundation 高通 CPU/GPU/DSP Snapdragon Neural Processing Engine SDK Apple CPU/GPU/NeualEngine CoreML ARM CPU/GPU ARM NN SDK MediaTek CPU/GPU/APU NeuroPilot SDK 4.2 软件厂商 工具 公司 系统支持情况 特点 TensorFlow Lite Google Android/IOS Android结合比较密，支持GPU加速 CoreML Apple IOS 软件硬件结合紧密，更新快 Caffe2 Facebook Android/IOS NCNN Tencent Android/IOS 支持大部分CNN网络，已经落地的应用多 MACE 小米 Android MNN 阿里 Android/IOS 利用Caffe2进行手写数字识别在Android端的部署 五、参考 [Paper] A Survey of Model Compression and Acceleration\nfor Deep Neural Networks [Book] 解析卷积神经网络\u0026ndash;深度学习实践手册 [Video] Toward Efficient Deep Neural Network Deployment: Deep Compression and EIE [Video] 李宏毅-Network Compression [Github] 模型压缩及移动端部署 ","permalink":"https://lyapple2008.github.io/posts/2019-08-04-%E4%BB%A3%E7%A0%81%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E8%AE%BA/","summary":"\u003cp\u003e\u003ca name=\"01ca666c\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"为什么需要代码优化\"\u003e为什么需要代码优化\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e提升体验，扩展玩法\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- ![image.png](https://user-images.githubusercontent.com/3350865/62418824-63b24f80-b6a5-11e9-9bde-6cd6de0cc6a5.PNG) --\u003e\n\u003cimg src=\"/images/2019-08-04-001.png\"/\u003e","title":"代码优化方法论"},{"content":"还记得刚毕业那会找工作，那是一定要找法工程师这个岗位，当时就只是觉得这个岗位牛逼是公司的核心岗位，但是可能对于算法工程师与一般的软件工程有什么区别其实心理也说不清楚。作为一个到今年6月份就工作满5年了的老菜鸟，这期间工作的title也是算法工程师（虽然有些并不是真的在做算法），反正在以算法工程师的title在企业工作的感受与毕业那会想像中上的算法工程师的工作完全不一样。\n我觉得所谓的算法工程师应该叫算法应用工程师会更合适一些。因为算法工程师的工作流程大概是这样的（以我自身的经历为例，可能不同公司有不同的工作方式），接到一个功能需求，然后开始调研实现这个功能需要的算法，研究过程中挑选两三种可能的方案，对这些挑选的方案进行实现（也可能是github clone），分析对比这几种方案。最后从中选出一种方案，作为最后的方案。接下来就是对最终的算法方案针对自己的功能需求和业务场景进行做优化，这期间你会对这个算法方案进行原理研究，参数调整，性能优化等等，最终的目标就是尽可能满足业务需求方的要求。\n知乎上@Jackpop的回答应该是符合大多数算法工程师的实际工作状态，而我们毕业那会想象中的算法工程师，天天手推工式那种，应该叫科学家或者研究员更合适些。总结一下，我觉得一个优秀的算法工程师应该是能够针对不同的业务场景选择最优的算法方案，并能对现有的算法针对业务场景做一些微调和优化，以更进一步适合业务场景。因为对于企业来说不能落地的算法，是丝毫没有价值的。\n以下转载自知乎：https://www.zhihu.com/question/310484101/answer/644079765\n算法工程师大致做什么的？ 算法，对于大多数理工科学生并不陌生。无论是学计算机还是学数学，或者其他理工科，我们都会接触很多成熟、经过十几年甚至几十年考验的算法。 算法工程师的关键点在“工程师”这三个字，日常所做的工作无非是选择一种或几种成熟、优秀的算法进行组合、验证，来解决特定场景下的问题。在大多数企业里面这一点体现更为明显，**在企业里作为算法工程师是不会创造算法，只是使用经过多年考验的成熟、稳定算法。**其实现在不仅是企业界，就连学术界，创新可用的新算法也是寥寥可数，就拿人工智能常用的优化算法来说：\n随机梯度 共轭梯度 牛顿法粒子群 遗传 贝叶斯 进化策略 这些算法每一个都是经过多年考验的，有的甚至几十年。 再拿计算机视觉来说：\nR-CNN系列 yolo系列 mobilenet \u0026hellip;. 目前大多数机器视觉算法工程师所做的工作基本也是围绕这些成熟的算法做微调，结合特定场景做迁移。**所以，作为算法工程师是不会创造新算法，作为算法工程师日常工作内容无非是根据具体的业务场景，根据自己的知识积累拿出几种成熟、好用的算法提出一个可用的解决方案，去解决业务上的问题。**所以很多答主所说的需要这样那样的知识，我觉得有点夸张了。\n有些答主说需要深入的数学知识，把算法说的神乎其神，作为本硕均为数学系的学生来说，对于日常算法，本科阶段的通识数学知识足够使用了，数学很有价值，但是过于脱离实际，实现难度也比较大，吴恩达在他的教学视频曾提到过共轭梯度法，吴恩达说“共轭梯度法效果不错，但是实现过程复杂，所以人工智能中很少被使用。”共轭梯度法在数学里面已经算是比较基础得了。\n算法工程师需要哪些能力？ 业务学习能力 算法工程师是不可能脱离业务背景的，人工智能算法工程师、交通算法工程师、图像处理算法工程师等等。 在针对一个业务场景设计一个合理的算法，业务知识是非常重要的，**需要结合业务的实际情况、限定条件、各种专业词汇和知识都要有一定的了解，如果脱离场景而一味地琢磨算法，效果不会太好，**比如，做交通算法，需要对交通组织、交通管理、通行损失、周期延误等有所认知。比如，做图像处理，需要对各种图像去噪、图像增广、图像分割、物理成像有所了解，知道像素底层是怎么回事。\n持续学习能力 就像我前面所说的**，算法工程师的主要工作就是拿着现有成熟的算法，结合面临业务场景去做一个合理的方案，如果我们知识面太窄，那显然当用到的时候会有点拮据，眼界也被限制住，不知道还有没有更好效果的算法、目前算法有哪些不足之处、在这个业务中能不能发挥作用，只有持续学习，了解足够多的知识，当我们面临问题的时候能够快速对比、选择，找出最合适的一种算法。**\n灵活的思维 当我们选择一种算法去解决一个问题时，效果肯定无法达到我们预期的那样，比如我们拿mask rcnn做医学图像语义分割，我们看着它在自然图像方面表现效果很好，就拿来用于医学图像，但是医学图像有它的难点和特殊性，当跑出效果时会发现结果不如人意，这时候就需要灵活的思维去发现问题，去调优、改进，或者从数据入手或者从网络模型入手或者从超参数入手。\n编程能力 不同公司对于算法工程师的定位有所差别，比如有些朋友在某公司算法工程师只负责方案的设计，开发由专门的开发人员实施。有的公司算法工程师要完成算法设计到开发全部工作。我认为无论是哪一种形式，编程能力都是必要的，就算是前者这样的形式，有专门的开发人员，那在算法的设计过程中需要验证、对比，对每一个小模块算法进行指标评价，你不可能事事都找别人来帮你做，这样效率低，而且开展工作困难。\n算法验证能力 就像前面提到的那样，**算法验证在算法工程师日常工作中占据很大的比重，**我们拿到一些成熟、优秀的算法后，它的效果如何？能否起作用是未知的，我们需要对它进行验证，包括效率、精度等方面。这就要求算法工程师拥有算法验证能力，能够在众多算法中挑选出一种合适的算法来解决相应问题。\n","permalink":"https://lyapple2008.github.io/posts/2019-04-07-%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%E5%88%B0%E5%BA%95%E6%98%AF%E5%81%9A%E4%BB%80%E4%B9%88%E7%9A%84/","summary":"\u003cp\u003e还记得刚毕业那会找工作，那是一定要找法工程师这个岗位，当时就只是觉得这个岗位牛逼是公司的核心岗位，但是可能对于算法工程师与一般的软件工程有什么区别其实心理也说不清楚。作为一个到今年6月份就工作满5年了的老菜鸟，这期间工作的title也是算法工程师（虽然有些并不是真的在做算法），反正在以算法工程师的title在企业工作的感受与毕业那会想像中上的算法工程师的工作完全不一样。\u003c/p\u003e","title":"算法工程师到底是做什么的"},{"content":"音乐人声分离概况 音乐人声分离的目的是，从一首歌曲中分离出人声和伴奏声。Project on Music/Voice Separation这个网站比较了一些基于传统方法的效果，但是由于基于了一些假设（比如REPET就假伴奏都是周期重复信号），这些算法在实际测试过程中效果都差强人意。随着深度学习的流行，音乐人声分离这个领域也开始被基于深度学习的方法所占领。在SiSEC MUS上可以看到效果比较好的都是基于深度学习的方法。\n基于深度学习方法的处理框架 - 训练数据 以DSD100数据集为例，下面是DSD100数据集的目录结构 ``` |--DSD100 |--Mixtures |--Dev |--051 - AM Contra - Heart Peripheral |--mixture.wav ==\u003e 混合音频 |--Test |--Sources |--Dev |--051 - AM Contra - Heart Peripheral |--bass.wav ==\u003e 各成分音频 |--drums.wav |--other.wav |--vocals.wav ==\u003e 人声部分音频 |--Test ``` - 训练流程 训练目标\n在本任务中的训练目标是对应输入混合音频幅度谱的一个Mask，通过这个Mask与混合音频幅度谱做mask乘法，就可以得到目标音频的幅度谱，再结合混合音频相位谱，做ISTFT就可以得到目标音频。下面是本任务的损失函数，其中X代表混合音频幅度谱，Y代表目标音频幅度谱，f(X,\u0026amp;)代表模型输出的Mask。 Idea Binary Mask\n理想二值掩蔽”（Ideal Binary Mask）中的分离任务就成为了一个二分类问题。这类方法根据听觉感知特性，把音频信号分成不同的子带，根据每个时频单元上的信噪比，把对应的时频单元的能量设为0（噪音占主导的情况下）或者保持原样（目标语音占主导的情况下）。\nIdea Ratio Mask\nIRM（Ideal Ratio Mask），它同样对每个时频单元进行计算，但不同于IBM的“非零即一”，IRM中会计算语音信号和噪音之间的能量比，得到介于0到1之间的一个数，然后据此改变时频单元的能量大小。IRM是对IBM的演进，反映了各个时频单元上对噪声的抑制程度，可以进一步提高分离后语音的质量和可懂度。\nDeep U-Net方法介绍 ``` 卷积层参数个数 = (kernel_w * kernel_h + 1) * output_channel 归一化层参数个数 = 4 * input_channel ; (gamma, beta, moving_mean, moving_variance) ``` Encode-Decode结构\nU-Net中使用的Encode-Decode结构有点类似于图像中的多尺度金字塔，在高分辨率图像中更多关注的是图像的细节纹理，而在低分辨率图像中则更多关注的是图像的轮廓信息，这样就可以对不同层级的信息进行分开建模。 跳跃连接（Skip-connection）\nskip-connection使得每个节点在进行建模时，能够同时利用到本层的信息和来自下一层的信息 相关尝试 More Skip-Connection UNet++网络结构将Skip-Connection发挥到了极致，每个节点不单单只跟同层相邻节点连接，而是跟同层所有节点进行相连。同时一个大的UNet++网络结构其内部又可以一层层拆分成小的UNet++网络结构，这个特点使得其在做剪枝优化的时候非常简单方便，如图UNet++ L4到UNet++ L1就是简化过程。\n由于UNet++相对于UNet结构增加了大概20%的参数，而训练数据并没有相应增强，最后的效果还没达到UNet的结果。\n结合相位信息 Deep Complex Network 实验验证相位在分离任务中的作用 方法 效果（1-10分，分数越高效果越好） 理想结果 10 基于幅度谱方法的结果 8 基于幅度谱和相位谱方法的结果 7 忽略相位的结果 0 基于幅度谱方法的理想结果 8 相位对于音源分类任务的影响极小，更准确的幅度谱估计才能提升效果，相位沿用混合音频的相位就好。\n一些可能的方向 WAVE U-Net直接在时域上进行分离 WAVE U-Net分离的结果中伴奏都很弱\nMulti-Scale Multi-band 借鉴图像分割领域的一些想法和进展\n对音频信号的短时幅度谱进行分割与对图像进行语义分割一个很大的不同在于，音频信号在幅度谱上是有位置差异的，也就是说同样形状的的信号在不同位置，它代表就是不同的音频内容；而在图像语义分割中，分割的对象是没有位置这个信息的，分割对象是可以存在于图像中不同的位置。 公开的数据集 名称 数据量 时长 年份 链接 DSD100 100首 6h58m20s 2015 https://sigsep.github.io/datasets/dsd100.html ccMixter 50首 3h12m30s 2014 https://members.loria.fr/ALiutkus/kam/ MUSDB18 150首 9h50m 2017 https://sigsep.github.io/datasets/musdb.html 参考 [1] Project on Music/Voice Separation: 一些传统方法\n[2] SigSep：关于音乐人声分离的公开数据集和开源实现方法 [3] Paper: Singing Voice Separation With Deep U-Net Convolution Networks\n[4] Paper: UNet++: A Nested U-Net Architecture for Medical Image Segmentation\n[5] Github: UNet-VocalSeparation-Chainer\n[6] Github: UNetPlusPlus\n[7] Github: Wave-U-Net [8] Github: Deep-Complex-Networks [9] Paper: Multi-Scale Multi-Band Densenets For Audio Source Separation [10] SigSep\u0026ndash;Tutorial\u0026ndash;\u0026ldquo;Music Source Separation with DNNs, Making it work\u0026rdquo;\n[11] 搜狗研究员讲解基于深度学习的语音分离\n[12] Paper: Phase-Aware Speech Enhancement With Deep Complex U-Net [13] SiSEC MUS: 音乐人声分离竞赛\n[14] Project on Music/Voice Separation: 一些传统方法\n","permalink":"https://lyapple2008.github.io/posts/2019-03-31-%E9%9F%B3%E4%B9%90%E4%BA%BA%E5%A3%B0%E5%88%86%E7%A6%BB%E5%B0%8F%E7%BB%93/","summary":"\u003ch3 id=\"音乐人声分离概况\"\u003e音乐人声分离概况\u003c/h3\u003e\n\u003cp\u003e音乐人声分离的目的是，从一首歌曲中分离出人声和伴奏声。\u003ca href=\"https://www.math.ucdavis.edu/~aberrian/research/voice_separation/index.html\"\u003eProject on Music/Voice Separation\u003c/a\u003e这个网站比较了一些基于传统方法的效果，但是由于基于了一些假设（比如REPET就假伴奏都是周期重复信号），这些算法在实际测试过程中效果都差强人意。随着深度学习的流行，音乐人声分离这个领域也开始被基于深度学习的方法所占领。在\u003ca href=\"http://www.sisec17.audiolabs-erlangen.de/#/results/1/4/2\"\u003eSiSEC MUS\u003c/a\u003e上可以看到效果比较好的都是基于深度学习的方法。\u003c/p\u003e","title":"音乐人声分离小结"},{"content":"记录一些音频开发过程中会用到的优质资源\nWebRTC WebRTC是google开源的实时音视频通讯项目，其中的AudioProcess模块包括了AGC/AEC/ANS算法，非常值得学习。同时WebRTC还是一个跨平台项目，代码中对各个平台（Android/Ios/Windows/Linux）硬件接口的封装和抽象可以直接拿来应用到自己的项目中。\nOboe Oboe是Google家开源一个高性能C++库，这个库封装了Android底层OpenSLES和AAudio接口，通过这个库可以方便地在Android实现Low Latency Audio，只需一套代码就可以了。\n","permalink":"https://lyapple2008.github.io/posts/2019-03-10-%E9%9F%B3%E9%A2%91%E5%BC%80%E5%8F%91%E8%B5%84%E6%BA%90/","summary":"\u003cp\u003e记录一些音频开发过程中会用到的优质资源\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://webrtc.org/\"\u003eWebRTC\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWebRTC是google开源的实时音视频通讯项目，其中的AudioProcess模块包括了AGC/AEC/ANS算法，非常值得学习。同时WebRTC还是一个跨平台项目，代码中对各个平台（Android/Ios/Windows/Linux）硬件接口的封装和抽象可以直接拿来应用到自己的项目中。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/google/oboe\"\u003eOboe\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003eOboe是Google家开源一个高性能C++库，这个库封装了Android底层OpenSLES和AAudio接口，通过这个库可以方便地在Android实现Low Latency Audio，只需一套代码就可以了。\u003c/p\u003e\u003c/blockquote\u003e","title":"音频开发资源"},{"content":"最近由于公司项目的原因开始接触WebRTC，其中Android相关部分由于需要跨越了两种不同的语 言，因此需要一种机制能够让C/C++和JAVA之间进行交互，而JNI就是这样一种机制。通过JNI可 以实现C/C++和JAVA之前需要交互。本篇笔记的首先从一个实际的例子开始介绍JNI操作的完整流 程是怎样的；接着将就这个例子完整介绍JNI中需要注意的点。 JNI操作完整流程介绍 编写Java类及声明Native方法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 public class Java2C { static { System.loadLibrary(\u0026#34;jni\u0026#34;); } // 成员变量 private int number = 88; private static double speed = 55.66; private String message = \u0026#34;Hello from Java.\u0026#34;; // 基本数据类型 private native double average(int n1, int n2); // 引用类型 private native String sayHello(String msg); // 基本数据类型数组 private native double[] sumAndAverage(int[] numbers); // 引用类型数组 private native String[] num2Str(int[] numbers); // 操作java成员变量 private native void modifyJavaVariable(); // java成员方法 private void callback(String message) { System.out.println(\u0026#34;In Java with \u0026#34; + message); } private double callbackAverage(int n1, int n2) { return ((double)n1 + n2) / 2.0; } // 静态成员方法 private static String callbackStatic() { return \u0026#34;From static Java method.\u0026#34;; } private native void testCallbackMethod(); public static void main(String[] args) { Java2C javaClass = new Java2C(); double aver = javaClass.average(5, 6); System.out.println(\u0026#34;primitive type: the average of 5 and 6 is \u0026#34; + aver); System.out.println(\u0026#34;Reference type: \u0026#34;); String javaString = javaClass.sayHello(\u0026#34;Hello From Java\u0026#34;); System.out.println(javaString); System.out.println(\u0026#34;primitive type array: \u0026#34;); int[] numbers = {11, 22, 33}; double[] results = javaClass.sumAndAverage(numbers); System.out.println(\u0026#34;In Java, the sum is \u0026#34; + results[0]); System.out.println(\u0026#34;In Java, the average is \u0026#34; + results[1]); System.out.println(\u0026#34;reference type array: \u0026#34;); String[] numStrs = javaClass.num2Str(numbers); System.out.println(\u0026#34;In Java the string is \u0026#34; + numStrs[0] + \u0026#34; \u0026#34; + numStrs[1] + \u0026#34; \u0026#34; + numStrs[2]); System.out.println(\u0026#34;Operate java variable: \u0026#34;); javaClass.modifyJavaVariable(); System.out.println(\u0026#34;C/C++　call java viarable and method\u0026#34;); javaClass.testCallbackMethod(); } } 生成jni native头文件 javac -h \u0026lt;头文件存放路径\u0026gt; \u0026lt;java类\u0026gt;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 /* DO NOT EDIT THIS FILE - it is machine generated */ #include \u0026lt;jni.h\u0026gt; /* Header for class Java2C */ #ifndef _Included_Java2C #define _Included_Java2C #ifdef __cplusplus extern \u0026#34;C\u0026#34; { #endif /* * Class: Java2C * Method: average * Signature: (II)D */ JNIEXPORT jdouble JNICALL Java_Java2C_average (JNIEnv *, jobject, jint, jint); /* * Class: Java2C * Method: sayHello * Signature: (Ljava/lang/String;)Ljava/lang/String; */ JNIEXPORT jstring JNICALL Java_Java2C_sayHello (JNIEnv *, jobject, jstring); /* * Class: Java2C * Method: sumAndAverage * Signature: ([I)[D */ JNIEXPORT jdoubleArray JNICALL Java_Java2C_sumAndAverage (JNIEnv *, jobject, jintArray); /* * Class: Java2C * Method: num2Str * Signature: ([I)[Ljava/lang/String; */ JNIEXPORT jobjectArray JNICALL Java_Java2C_num2Str (JNIEnv *, jobject, jintArray); /* * Class: Java2C * Method: modifyJavaVariable * Signature: ()V */ JNIEXPORT void JNICALL Java_Java2C_modifyJavaVariable (JNIEnv *, jobject); /* * Class: Java2C * Method: testCallbackMethod * Signature: ()V */ JNIEXPORT void JNICALL Java_Java2C_testCallbackMethod (JNIEnv *, jobject); #ifdef __cplusplus } #endif #endif 在C文件中编写相关jni native方法的实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 #include \u0026lt;jni.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026#34;Java2C.h\u0026#34; JNIEXPORT jdouble JNICALL Java_Java2C_average (JNIEnv *env, jobject thisObj, jint n1, jint n2) { jdouble result; printf(\u0026#34;In C, the numbers are %d and %d\\n\u0026#34;, n1, n2); result = ((jdouble)n1 + n2) / 2.0; return result; } JNIEXPORT jstring JNICALL Java_Java2C_sayHello (JNIEnv *env, jobject thisObj, jstring inJNIStr) { const char *inCStr = (*env)-\u0026gt;GetStringUTFChars(env, inJNIStr, NULL); if (NULL == inCStr) return NULL; printf(\u0026#34;In C, the receiving string is: %s\\n\u0026#34;, inCStr); (*env)-\u0026gt;ReleaseStringUTFChars(env, inJNIStr, inCStr); char outCStr[128] = \u0026#34;Out string from C\u0026#34;; return (*env)-\u0026gt;NewStringUTF(env, outCStr); } JNIEXPORT jdoubleArray JNICALL Java_Java2C_sumAndAverage (JNIEnv *env, jobject thisObj, jintArray inJNIArray) { jint *inCArray = (*env)-\u0026gt;GetIntArrayElements(env, inJNIArray, NULL); if (NULL == inCArray) return NULL; jsize length = (*env)-\u0026gt;GetArrayLength(env, inJNIArray); jint sum = 0; int i; for (i = 0;i\u0026lt;length;i++) { sum += inCArray[i]; } jdouble average = (jdouble)sum / length; (*env)-\u0026gt;ReleaseIntArrayElements(env, inJNIArray, inCArray, 0); jdouble outArray[] = {sum, average}; jdoubleArray outJNIArray = (*env)-\u0026gt;NewDoubleArray(env, 2); if (NULL == outJNIArray) return NULL; (*env)-\u0026gt;SetDoubleArrayRegion(env, outJNIArray, 0, 2, outArray); return outJNIArray; } JNIEXPORT jobjectArray JNICALL Java_Java2C_num2Str (JNIEnv *env, jobject thisObj, jintArray inJNIArray) { jobjectArray ret; jint *inCArray = (*env)-\u0026gt;GetIntArrayElements(env, inJNIArray, NULL); if (NULL == inCArray) return NULL; jsize length = (*env)-\u0026gt;GetArrayLength(env, inJNIArray); char cStrArray[6][128]; int i; for (i = 0;i \u0026lt; length; i++) { snprintf(cStrArray[i], 128, \u0026#34;%d\u0026#34;, inCArray[i]); } jclass strClass = (*env)-\u0026gt;FindClass(env, \u0026#34;java/lang/String\u0026#34;); ret = (*env)-\u0026gt;NewObjectArray(env, length, strClass, (*env)-\u0026gt;NewStringUTF(env, \u0026#34;\u0026#34;)); for (i=0;i\u0026lt;length;i++) { (*env)-\u0026gt;SetObjectArrayElement(env, ret, i, (*env)-\u0026gt;NewStringUTF(env, cStrArray[i])); } return ret; } JNIEXPORT void JNICALL Java_Java2C_modifyJavaVariable (JNIEnv *env, jobject thisObj) { jclass thisClass = (*env)-\u0026gt;GetObjectClass(env, thisObj); jfieldID fidNumber = (*env)-\u0026gt;GetFieldID(env, thisClass, \u0026#34;number\u0026#34;, \u0026#34;I\u0026#34;); if (NULL == fidNumber) return; jint number = (*env)-\u0026gt;GetIntField(env, thisObj, fidNumber); printf(\u0026#34;In C, thi int is %d\\n\u0026#34;, number); number = 99; (*env)-\u0026gt;SetIntField(env, thisObj, fidNumber, number); jfieldID fidMessage = (*env)-\u0026gt;GetFieldID(env, thisClass, \u0026#34;message\u0026#34;, \u0026#34;Ljava/lang/String;\u0026#34;); if (NULL == fidMessage) return ; jstring message = (*env)-\u0026gt;GetObjectField(env, thisObj, fidMessage); const char* cStr = (*env)-\u0026gt;GetStringUTFChars(env, message, NULL); if (NULL == cStr) return ; printf(\u0026#34;In C, the string is %s\\n\u0026#34;, cStr); (*env)-\u0026gt;ReleaseStringUTFChars(env, message, cStr); message = (*env)-\u0026gt;NewStringUTF(env, \u0026#34;Hello from C\u0026#34;); if (NULL == message) return ; (*env)-\u0026gt;SetObjectField(env, thisObj, fidMessage, message); jfieldID fidSpeed = (*env)-\u0026gt;GetStaticFieldID(env, thisClass, \u0026#34;speed\u0026#34;, \u0026#34;D\u0026#34;); if (NULL == fidSpeed) return ; jdouble speed = (*env)-\u0026gt;GetStaticDoubleField(env, thisClass, fidSpeed); printf(\u0026#34;In C, the speed is %f\\n\u0026#34;, speed); speed = 77.99; (*env)-\u0026gt;SetStaticDoubleField(env, thisClass, fidSpeed, speed); } JNIEXPORT void JNICALL Java_Java2C_testCallbackMethod (JNIEnv *env, jobject thisObj) { jclass thisClass = (*env)-\u0026gt;GetObjectClass(env, thisObj); jmethodID midCallback = (*env)-\u0026gt;GetMethodID(env, thisClass, \u0026#34;callback\u0026#34;, \u0026#34;(Ljava/lang/String;)V\u0026#34;); if (NULL == midCallback) return ; printf(\u0026#34;In C, call back Java\u0026#39;s callback(String)\\n\u0026#34;); jstring message = (*env)-\u0026gt;NewStringUTF(env, \u0026#34;Hello from C\u0026#34;); (*env)-\u0026gt;CallVoidMethod(env, thisObj, midCallback, message); jmethodID midCallbackAverage = (*env)-\u0026gt;GetMethodID(env, thisClass, \u0026#34;callbackAverage\u0026#34;, \u0026#34;(II)D\u0026#34;); if (NULL == midCallbackAverage) return ; printf(\u0026#34;In C, call back Java\u0026#39;s callbackAverage\\n\u0026#34;); jdouble average = (*env)-\u0026gt;CallDoubleMethod(env, thisObj, midCallbackAverage, 2, 3); printf(\u0026#34;In C, the average is %f\\n\u0026#34;, average); jmethodID midCallbackStatic = (*env)-\u0026gt;GetStaticMethodID(env, thisClass, \u0026#34;callbackStatic\u0026#34;, \u0026#34;()Ljava/lang/String;\u0026#34;); if (NULL == midCallbackStatic) { printf(\u0026#34;1111\\n\u0026#34;); return ; } jstring resultJNIStr = (*env)-\u0026gt;CallStaticObjectMethod(env, thisClass, midCallbackStatic); if (NULL == resultJNIStr) return ; const char* resultCStr = (*env)-\u0026gt;GetStringUTFChars(env, resultJNIStr, NULL); if (NULL == resultCStr) return; printf(\u0026#34;In C, the returned string is %s\\n\u0026#34;, resultCStr); } 通过gcc/g++ 生成动态库 gcc -fPIC -I\u0026quot;$JAVA_HOME/include\u0026quot; -I\u0026quot;$JAVA_HOME/include/linux\u0026quot; -shared -o libjni.so Java2C.c\n运行进行测试 java -Djava.library.path=. Java2C 这里需要使用-D指定前面生的动态库路径，否则运行的时候会提示找不到动态库的错误\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 In C, the numbers are 5 and 6 primitive type: the average of 5 and 6 is 5.5 Reference type: In C, the receiving string is: Hello From Java Out string from C primitive type array: In Java, the sum is 66.0 In Java, the average is 22.0 reference type array: In Java the string is 11 22 33 Operate java variable: In C, thi int is 88 In C, the string is Hello from Java. In C, the speed is 55.660000 C/C++　call java viarable and method In C, call back Java\u0026#39;s callback(String) In Java with Hello from C In C, call back Java\u0026#39;s callbackAverage In C, the average is 2.500000 In C, the returned string is From static Java method. JNI中类型的对应关系与转换 基于数据类型 Java类型 Native Type 描述 boolean jboolean C/C++无符号的8位整型（unsigned char） byte jbyte C/C++带符号的8位整型（char） char jchar C/C++无符号的16位整型（unsigned short） short jshort C/C++带符号的16位整型 （signed short) int jint C/C++带符号的32位整型（int） long jlong C/C++带符号的64位整型（long） float jfloat C/C++32位浮点型（float） double jdouble C/C++64位浮点型（double） 引用数据类型 Java类型 Native Type 描述 Object jobject 任何Java对象，或者没有对应java类型的对象 Class jclass Class类对象 String jstring 字符串对象 Object[] jobjectArray 任何对象的数组 boolean[] jbooleanArray 布尔型数组 byte[] jbyteArray 比特型数组 char[] jcharArray 字符型数组 short[] jshortArray 短整型数组 int[] jintArray 整型数组 long[] jlongArray 长整型数组 float[] jfloatArray 单精度浮点型数组 double[] jdouble 双精度浮点型数组 void void n/a 引用类型的继承关系\n注意 基本数据类型可以在native层直接使用 引用数据类型则不能直接使用，需要根据JNI函数进行相应的转换才能使用 多维数据（包括二维数组）都是引用类型，需要使用jobjectArray类型存取其值 JNI相关操作方法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 // native string方法 // UTF-8 String (encoded to 1-3 byte, backward compatible with 7-bit ASCII) // Can be mapped to null-terminated char-array C-string const char * GetStringUTFChars(JNIEnv *env, jstring string, jboolean *isCopy); // Returns a pointer to an array of bytes representing the string in modified UTF-8 encoding. void ReleaseStringUTFChars(JNIEnv *env, jstring string, const char *utf); // Informs the VM that the native code no longer needs access to utf. jstring NewStringUTF(JNIEnv *env, const char *bytes); // Constructs a new java.lang.String object from an array of characters in modified UTF-8 encoding. jsize GetStringUTFLength(JNIEnv *env, jstring string); // Returns the length in bytes of the modified UTF-8 representation of a string. void GetStringUTFRegion(JNIEnv *env, jstring str, jsize start, jsize length, char *buf); // Translates len number of Unicode characters beginning at offset start into modified UTF-8 encoding // and place the result in the given buffer buf. // Unicode Strings (16-bit character) const jchar * GetStringChars(JNIEnv *env, jstring string, jboolean *isCopy); // Returns a pointer to the array of Unicode characters void ReleaseStringChars(JNIEnv *env, jstring string, const jchar *chars); // Informs the VM that the native code no longer needs access to chars. jstring NewString(JNIEnv *env, const jchar *unicodeChars, jsize length); // Constructs a new java.lang.String object from an array of Unicode characters. jsize GetStringLength(JNIEnv *env, jstring string); // Returns the length (the count of Unicode characters) of a Java string. void GetStringRegion(JNIEnv *env, jstring str, jsize start, jsize length, jchar *buf); // Copies len number of Unicode characters beginning at offset start to the given buffer buf 1 2 3 4 5 6 7 8 9 10 11 // JNI Primitive Array Function // ArrayType: jintArray, jbyteArray, jshortArray, jlongArray, jfloatArray, jdoubleArray, jcharArray, jbooleanArray // PrimitiveType: int, byte, short, long, float, double, char, boolean // NativeType: jint, jbyte, jshort, jlong, jfloat, jdouble, jchar, jboolean NativeType * Get\u0026lt;PrimitiveType\u0026gt;ArrayElements(JNIEnv *env, ArrayType array, jboolean *isCopy); void Release\u0026lt;PrimitiveType\u0026gt;ArrayElements(JNIEnv *env, ArrayType array, NativeType *elems, jint mode); void Get\u0026lt;PrimitiveType\u0026gt;ArrayRegion(JNIEnv *env, ArrayType array, jsize start, jsize length, NativeType *buffer); void Set\u0026lt;PrimitiveType\u0026gt;ArrayRegion(JNIEnv *env, ArrayType array, jsize start, jsize length, const NativeType *buffer); ArrayType New\u0026lt;PrimitiveType\u0026gt;Array(JNIEnv *env, jsize length); void * GetPrimitiveArrayCritical(JNIEnv *env, jarray array, jboolean *isCopy); void ReleasePrimitiveArrayCritical(JNIEnv *env, jarray array, void *carray, jint mode); C/C++调用Java类中的成员变量 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // JNI操作成员变量变量 jclass GetObjectClass(JNIEnv *env, jobject obj); // Returns the class of an object. jfieldID GetFieldID(JNIEnv *env, jclass cls, const char *name, const char *sig); // Returns the field ID for an instance variable of a class. NativeType Get\u0026lt;type\u0026gt;Field(JNIEnv *env, jobject obj, jfieldID fieldID); void Set\u0026lt;type\u0026gt;Field(JNIEnv *env, jobject obj, jfieldID fieldID, NativeType value); // Get/Set the value of an instance variable of an object // \u0026lt;type\u0026gt; includes each of the eight primitive types plus Object. jfieldID GetStaticFieldID(JNIEnv *env, jclass cls, const char *name, const char *sig); // Returns the field ID for a static variable of a class. NativeType GetStatic\u0026lt;type\u0026gt;Field(JNIEnv *env, jclass clazz, jfieldID fieldID); void SetStatic\u0026lt;type\u0026gt;Field(JNIEnv *env, jclass clazz, jfieldID fieldID, NativeType value); // Get/Set the value of a static variable of a class. // \u0026lt;type\u0026gt; includes each of the eight primitive types plus Object. C/C++调用Java类中的成员方法 通过下面这条命令可以获取到java中方法的签名\njavas -s -p Java2C\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 Compiled from \u0026#34;Java2C.java\u0026#34; public class Java2C { private int number; descriptor: I private static double speed; descriptor: D private java.lang.String message; descriptor: Ljava/lang/String; public Java2C(); descriptor: ()V private native double average(int, int); descriptor: (II)D private native java.lang.String sayHello(java.lang.String); descriptor: (Ljava/lang/String;)Ljava/lang/String; private native double[] sumAndAverage(int[]); descriptor: ([I)[D private native java.lang.String[] num2Str(int[]); descriptor: ([I)[Ljava/lang/String; private native void modifyJavaVariable(); descriptor: ()V private void callback(java.lang.String); descriptor: (Ljava/lang/String;)V private double callbackAverage(int, int); descriptor: (II)D private static java.lang.String callbackStatic(); descriptor: ()Ljava/lang/String; private native void testCallbackMethod(); descriptor: ()V public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V static {}; descriptor: ()V } 再通过下面这些JNI方法变可以在C/C++代码中调用到JAVA类的成员方法了\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 jmethodID GetMethodID(JNIEnv *env, jclass cls, const char *name, const char *sig); // Returns the method ID for an instance method of a class or interface. NativeType Call\u0026lt;type\u0026gt;Method(JNIEnv *env, jobject obj, jmethodID methodID, ...); NativeType Call\u0026lt;type\u0026gt;MethodA(JNIEnv *env, jobject obj, jmethodID methodID, const jvalue *args); NativeType Call\u0026lt;type\u0026gt;MethodV(JNIEnv *env, jobject obj, jmethodID methodID, va_list args); // Invoke an instance method of the object. // The \u0026lt;type\u0026gt; includes each of the eight primitive and Object. jmethodID GetStaticMethodID(JNIEnv *env, jclass cls, const char *name, const char *sig); // Returns the method ID for an instance method of a class or interface. NativeType CallStatic\u0026lt;type\u0026gt;Method(JNIEnv *env, jclass clazz, jmethodID methodID, ...); NativeType CallStatic\u0026lt;type\u0026gt;MethodA(JNIEnv *env, jclass clazz, jmethodID methodID, const jvalue *args); NativeType CallStatic\u0026lt;type\u0026gt;MethodV(JNIEnv *env, jclass clazz, jmethodID methodID, va_list args); // Invoke an instance method of the object. // The \u0026lt;type\u0026gt; includes each of the eight primitive and Object. 参考 [1] Java Programming Tutorial Java Native Interface (JNI)\n","permalink":"https://lyapple2008.github.io/posts/2019-03-03-javanativeinterface%E7%AC%94%E8%AE%B0/","summary":"\u003cp\u003e最近由于公司项目的原因开始接触WebRTC，其中Android相关部分由于需要跨越了两种不同的语\n言，因此需要一种机制能够让C/C++和JAVA之间进行交互，而JNI就是这样一种机制。通过JNI可\n以实现C/C++和JAVA之前需要交互。本篇笔记的首先从一个实际的例子开始介绍JNI操作的完整流\n程是怎样的；接着将就这个例子完整介绍JNI中需要注意的点。\n\u003cimg alt=\"image\" loading=\"lazy\" src=\"https://user-images.githubusercontent.com/3350865/61180103-91c2e780-a643-11e9-8132-f85c7a83f8da.jpg\"\u003e\n\u003cimg src=\"/images/2019-03-03-001.jpg\"\u003e\u003c/p\u003e","title":"JavaNativeInterface笔记"},{"content":"2018回顾 先看下2018初始的时候自己立下的flag吧，看看脸被打得有多痛\n咱们一个脸一个脸地来打，轻拍：\n健身 最近一次记录的体重是64.3Kg，距离55Kg还是有很大距离，初始体重是65.9Kg 这一项完成度是14.68%\n跑里程达到400Km，这里有记录的数据是8.20Km 太可怜了，我不算了，直接写**0%**好了\n小爱好 现在连一首\u0026laquo;纸短情长\u0026raquo;都还没练会，太笨了。。。。 完成度是0%\n今年书是看了《小岛经济学》《九败一胜：美团创始人王兴》《跃迁：成为高手的技术》 这个完成度也算是0%，没有输出的阅读，相当于是无效的努力\n技术 音视频应用开发：这个也没有继续跟进下去，然后最后还是逃不掉\n深度学习方面：由于新工作的原因，开始接触实际项目，算是个入门吧\n写博客 2018一共写了7篇文章，也是一个远不及格的成绩，没有输出相当于没有成长。\n2019新的Flag 虽然脸已经被打肿了，但是flag还是不能倒呀，2019-猪年-新的flag还是要立起来。通过回顾2018年flag的完成情况，可以深刻的把自己定位为一个执行力弱不爆的人士了。作为一个立志成为海贼王的男人，怎么可以这样呢，2019必须成长了呀，才能成为海贼王的男人。在成为海贼王的男人前，先把2019年的flag完成好吗，脸真的痛好吗\n健身 体重降到58Kg 小爱好 录制《纸短情长ukelele》的小视频 非技术读书笔记2篇 海贼王系列视频特效准备 技术 WebRTC音视频通讯开发 输出一篇移动端布署深度学习应用的文章 写博客 每月一篇博客，不限主题 ","permalink":"https://lyapple2008.github.io/posts/2019-01-26-2019%E5%B0%8F%E7%9B%AE%E6%A0%87/","summary":"\u003ch1 id=\"2018回顾\"\u003e2018回顾\u003c/h1\u003e\n\u003cp\u003e先看下2018初始的时候自己立下的flag吧，看看脸被打得有多痛\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image\" loading=\"lazy\" src=\"https://user-images.githubusercontent.com/3350865/61180087-427cb700-a643-11e9-8bfd-354561f9d0e2.jpg\"\u003e\n\u003cimg src=\"/images/2019-01-26-001.jpg\" /\u003e\u003c/p\u003e","title":"2019小目标--重新开始打脸之路"},{"content":" 收集一些音频相关的公司和实验室，另外还有一些个人博客，用来了解行业的最新动态。\n技术公司 smule 一些乐器应用挺有意思的 SoundHound 哼唱歌曲识别 Vocaloid 歌声合成 Gracenote 实验室 Interactive audio lab Music Technology Group Neural Acoustic Processing Lab Music and Audio Research Group, Seoul National University 个人博客 Melody Extraction Yun Wang (Maigo) 开源库 Essentia ","permalink":"https://lyapple2008.github.io/posts/2018-10-29-%E6%94%B6%E9%9B%86%E9%9F%B3%E9%A2%91%E6%8A%80%E6%9C%AF%E5%BA%94%E7%94%A8%E5%85%AC%E5%8F%B8%E4%B8%8E%E5%AE%9E%E9%AA%8C%E5%AE%A4/","summary":"\u003c!-- ![](https://user-images.githubusercontent.com/3350865/61180076-fe89b200-a642-11e9-81a7-697446a0860a.jpg)    --\u003e\n\u003cp\u003e\u003cimg alt=\"Image Caption\" loading=\"lazy\" src=\"/images/2018-10-29-001.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e  收集一些音频相关的公司和实验室，另外还有一些个人博客，用来了解行业的最新动态。\u003c/p\u003e","title":"收集音频技术应用公司与实验室"},{"content":" 今天又一次重新开始健身计划，这次到这里来做个记录，看看能坚持多久！为了更好的身体，为了腹肌，加油，少年，你是要成为海贼王的男人！！！\n器械：瑜伽垫一张 课程：Fit健身 今天买了课程，花了大价钱，达不成目标就要打水漂了！！！\n十月运动记录 日期 体重（Kg） 运动内容 运动时间（分钟） 2018-10-15 65.9 HIIT-畅快流汗 20 2018-10-16 65.0 HIIT-畅快流汗 20 2018-10-17 65.3 HIIT-腹部燃脂 29 2018-10-18 65.1 HIIT-脂肪ByeBye 14 2018-10-19 2018-10-20 2018-10-21 64.7 FIT健身体能测试 10 2018-10-22 65.4 腹肌撕裂者初级\nHIIT-进击的Burpee 25 2018-10-23 65.7 打篮球 60 2018-10-24 65.4 休息恢复 2018-10-25 65.6 HIIT-腹部燃脂 29 2018-10-26 2018-10-27 HIIT-全身活力燃脂 22 2018-10-28 64.1 HIIT-畅快流汗\n入门瑜伽伸展 30 2018-10-29 65.1 核心力量初级训练\nTabata-碎脂机进阶 24 2018-10-30 65.3 打篮球 60 2018-10-26 运动天数 开始体重 结束体重 体重变化（Kg） 13 65.9 65.3 -0.6 十一月运动记录 日期 体重（Kg） 运动内容 运动时间（分钟） 2018-11-1 64.7 腹肌撕裂者初级\nHIIT-减脂巨星 41 2018-11-2 2018-11-3 2018-11-4 2018-11-5 64.6 HIIT-畅快流汗\n核心进阶减脂训练 36 2018-11-6 64.3 强力带全身焕活-减脂初级 49 2018-11-7 2018-11-8 64.7 HIIT-全身活力燃脂\n腹肌撕裂者进阶 40 2018-11-9 2018-11-10 2018-11-11 2018-11-12 64.1 Tabata-碎脂机进阶\n核心进阶减脂训练 29 2018-11-13 64.2 打篮球 60 2018-11-14 2018-11-15 64.6 HIIT-腹部燃脂 28 2018-11-16 2018-11-17 2018-11-18 64.6 十分鐘家中腹肌訓練 10 2018-11-19 64.7 HIIT-进击的Burpee 22 2018-11-20 64.3 打篮球 60 2018-11-21 64.4 HIIT-腹部燃脂 29 2018-11-22 2018-11-23 2018-11-24 2018-11-25 2018-11-26 2018-11-27 63.6 打篮球 60 2018-11-28 63.7 十分鐘家中腹肌訓練 10 2018-11-29 63.8 HIIT-全身活力燃脂 22 运动天数 开始体重 结束体重 体重变化（Kg） 14 65.3 63.8 -1.5 十二月运动记录 日期 体重（Kg） 运动内容 运动时间（分钟） 2018-12-1 2018-12-2 2018-12-3 2018-12-4 64.4 打篮球 60 2018-12-5 63.9 腹肌撕裂者进阶 18 2018-12-6 64.0 HIIT-进击的Burpee 22 2018-12-7 2018-12-8 2018-12-9 2018-12-10 63.6 十分鐘家中腹肌訓練 10 2018-12-11 64.3 打篮球 60 2018-12-12 2018-12-13 2018-12-14 2018-12-15 2018-12-16 2018-12-17 64.3 HIIT-减脂巨星 27 爆开始前的照片 ","permalink":"https://lyapple2008.github.io/posts/2018-10-15-2018%E5%81%A5%E8%BA%AB%E8%AE%B0%E5%BD%95/","summary":"\u003cp\u003e  今天又一次重新开始健身计划，这次到这里来做个记录，看看能坚持多久！为了更好的身体，为了腹肌，加油，少年，你是要成为海贼王的男人！！！\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e器械：瑜伽垫一张\u003c/li\u003e\n\u003cli\u003e课程：Fit健身\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003e今天买了课程，花了大价钱，达不成目标就要打水漂了！！！\u003c/p\u003e","title":"2018健身记录"},{"content":" 在踩了无数坑之后，才有了这篇文章，现在就一个想法还是Linux系统好呀。编译什么东西都是一个套路，configure/make/make install。之前不知道为什么那么程序员会推崇用mac做开发，编译的这个问题上Mac绝对完胜Windows。\n一、需要下载的工具 Visual Studio 2013 或 Visual Studio 2015 MSYS2 msys2-x86_64 YASM Win64.exe FFmpeg源码 ffmpeg4.0.1 二、编译环境搭建 1. 安装MSYS2，默认安装在C:\\msys64目录，安装完成后打开msys2环境，运行命令安装编译工具 pacman -S make gcc diffutils\n注：安装过程中可能会出现某些包下载失败的情况，这时只需要多试几次就好，直到提示净更新大小为0就说明安装完整了。\n2. 修改C:\\msys64\\msys2_shell.cmd文件，修改下图红框里的部分，这里是为了让msys2能继承VS的环境变量 set MSYS2_PATH_TYPE=inherit\n3. 将C:\\msys64\\usr\\bin\\link.exe重命名为其它名字，如link_bak.exe，这里是为了在链接的时候选择Visual Studio的link.exe工具，避免冲突 4. 将下载的yasm工具，yasm-1.3.0-win32.exe，复制到C:\\msys64\\usr\\bin目录，将重命令为yasm.exe 三、准备编译 1. 打开visual studio命令行，这里要选择vs2015 x64 x86兼容工具提示符，打开方式如下图所示 2. 切换目录到C:\\msys64，运行msy2_shell.cmd，开启msys2环境 3. 测试环境是否正确，检查编译工具和链接工具是不是指向的VS工具 四、开始编译（以下命令均在msys2环境执行） 1. 切换目录到ffmpeg源码目录 2. 运行configure脚本，相关参数参考下面，configure脚本执行的时间会比较长，耐心等待 1 2 3 4 5 6 7 8 9 10 ./configure \\ --prefix=./build_hwaccel \\ ====\u0026gt; 【可选】这里指定编译完成后库的安装目录 --toolchain=msvc \\ ====\u0026gt; 【必须】选择使用VS的工具和环境 --arch=x86 \\ ====\u0026gt; 【必须】x86指定生成win32版本的库 --enable-x86asm \\ ====\u0026gt; 【可选】使能x86的汇编优化 --disable-doc \\ ====\u0026gt; 【可选】关闭ffmpeg文档输出 --enable-shared \\ ====\u0026gt; 【必须】设置输出动态库 --disable-static \\ ====\u0026gt; 【可选】设置关闭输出静态库 --enable-d3d11va \\ ====\u0026gt; 【可选】设置使能d3d11硬件加速 --enable-dxva2 ====\u0026gt; 【可选】设置便能dxva2接口的硬件加速 3. 执行make命令，这个过程可能会有一些错误提示，可以根据提示上网进搜索相关问题， 如编译ffmpeg4.0.1需要注释与变量CC_IDENT相关的语句，存在于cmdutils.c和ff_probe.c文件中\n4. 执行make install命令，将生成的动态库复制到configure时指定的目录中 5. 至此整个windows下的ffmpeg动态库编译完成 ","permalink":"https://lyapple2008.github.io/posts/2018-08-18-windows%E4%B8%8B%E8%BD%BD%E7%BC%96%E8%AF%91ffmpeg%E5%8A%A8%E6%80%81%E5%BA%93%E6%8C%87%E5%8C%97/","summary":"\u003cp\u003e  在踩了无数坑之后，才有了这篇文章，现在就一个想法还是Linux系统好呀。编译什么东西都是一个套路，configure/make/make install。之前不知道为什么那么程序员会推崇用mac做开发，编译的这个问题上Mac绝对完胜Windows。\u003c/p\u003e","title":"Windows下载编译FFmpeg动态库指北"},{"content":" 对DeepLearning最初的印象是，大量的训练样本+机器学习，也就是说原来传统的机器学习会遇到的问题，不能解决的问题，换成DeepLearning同样解决不了。比如目标识别中因为光照变化，目标被遮挡，目标的几何变化造成的识别率大幅下降，在DeepLearning中同样也不能很好解决。但是不是说DeepLearning就一无事处，最近几年这么热也决不是因为名字取得好。DeepLearning比较明显的优势就是在特征选择上，想想之前做生物特征识别时，各种找特征，还得考虑什么光照不变，旋转不变，抗尺寸变换，抗遮挡，那叫一个累呀。现在可好啦，一个Convolution Layer，再配Fully Connected Layer，最后来个Softmax，丢一堆带标签的样本进去自动给你找出特征。当然这个只是一个接触DeepLearning不到一个月的小白的肤浅认识，大家听听就好。\n本文算是最近1个月学习DeepLearning的入门小作业，选择的例子也是DeepLearning最流行的HelloWorld程序MNIST手写数字识别，采用Caffe2进行训练，并在Android端实现一个Demo样例。\nDeepLearning基础 这里推荐下台大李宏毅老师的DeepLearning课程，讲解风趣幽默，生动详细，力荐。B站链接 深度学习框架：caffe2\n模型训练 整个模型训练过程主要包括数据准备、模型建立、模型训练，参考caffe2官网的tutorial\n1. 数据准备 数据准备在MachineLearning类的应用中起到致关重要的作用，相当于煮饭的时候用到的米。再利害的算法，如果没有足够的数据，那也是巧妇难为无米之炊，难道马云会称现在是DT时代。另外DeepLearning作为MachineLearning的一个分支，目前了解到的大部分的DeepLearning算法更多的还是属于SuperviseLearning。SuperviseLearning一个明显的特征是非常依赖数据，而且是人工标注的数据，这也难怪一些DeepLearning大大们说在AI应用中，有多少人工就有多少智能。本文用到的数据链接地址：MNIST手写数字数据集\n2. 模型建立 这个例子采用的是DeepLearning中的经典网络LeNet，关于LeNet可以参考这篇文章。\n3. 模型训练 这里模型训练采用caffe2框架，\n模型在Android端的部署 参考caffe2官网给的AICamera例子，建立Android Studio工程（github工程地址：https://github.com/lyapple2008/MNIST_CNN_APP ），其中最主要的代码如下所示\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 void loadToNetDef(AAssetManager *mgr, caffe2::NetDef *net, const char *filename) { AAsset *asset = AAssetManager_open(mgr, filename, AASSET_MODE_BUFFER); assert(asset != nullptr); const void *data = AAsset_getBuffer(asset); assert(data != nullptr); off_t len = AAsset_getLength(asset); assert(len != 0); if (!net-\u0026gt;ParseFromArray(data, len)) { alog(\u0026#34;Couldn\u0026#39;t parse net from data.\\n\u0026#34;); } AAsset_close(asset); } extern \u0026#34;C\u0026#34; void Java_com_example_beyoung_handwrittendigit_MainActivity_initCaffe2( JNIEnv *env, jobject, jobject assetManager) { AAssetManager *mgr = AAssetManager_fromJava(env, assetManager); alog(\u0026#34;Attempting to load protobuf netdefs...\u0026#34;); loadToNetDef(mgr, \u0026amp;_initNet, \u0026#34;mnist/init_net.pb\u0026#34;); loadToNetDef(mgr, \u0026amp;_predictNet, \u0026#34;mnist/predict_net.pb\u0026#34;); alog(\u0026#34;done.\u0026#34;); alog(\u0026#34;Instantiating predictor...\u0026#34;); _predictor = new caffe2::Predictor(_initNet, _predictNet); if (_predictor) { alog(\u0026#34;done...\u0026#34;); } else { alog(\u0026#34;fail to instantiat predictor...\u0026#34;); } } extern \u0026#34;C\u0026#34; JNIEXPORT jstring JNICALL Java_com_example_beyoung_handwrittendigit_MainActivity_recognitionFromCaffe2( JNIEnv *env, jobject, jint h, jint w, jintArray data) { if (!_predictor) { return env-\u0026gt;NewStringUTF(\u0026#34;Loading...\u0026#34;); } jsize len = env-\u0026gt;GetArrayLength(data); jint *img_data = env-\u0026gt;GetIntArrayElements(data, 0); jint img_size = h * w; assert(img_size \u0026lt;= INPUT_DATA_SIZE); // convert rgb image to grey image and normalize to 0~1 for (auto i = 0; i \u0026lt; h; ++i) { std::ostringstream stringStream; for (auto j = 0; j \u0026lt; w; ++j) { int color = img_data[i * w + j]; //int red = ((color \u0026amp; 0x00FF0000) \u0026gt;\u0026gt; 16); //int green = ((color \u0026amp; 0x0000FF00) \u0026gt;\u0026gt; 8); //int blue = color \u0026amp; 0x000000FF; //float grey = red * 0.3 + green * 0.59 + blue * 0.11; float grey = 0.0; if (color != 0) { grey = 1.0; } input_data[i * w + j] = grey; //alog(\u0026#34;%f\u0026#34;, grey); //alog(\u0026#34;%d\u0026#34;, color); if (color != 0) { color = 1; } stringStream \u0026lt;\u0026lt; color \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } //alog(\u0026#34;\\n\u0026#34;); alog(\u0026#34;%s\u0026#34;, stringStream.str().c_str()); } caffe2::TensorCPU input; input.Resize(std::vector\u0026lt;int\u0026gt;({1, IMG_C, IMG_H, IMG_W})); memcpy(input.mutable_data\u0026lt;float\u0026gt;(), input_data, INPUT_DATA_SIZE * sizeof(float)); caffe2::Predictor::TensorVector input_vec{\u0026amp;input}; caffe2::Predictor::TensorVector output_vec; _predictor-\u0026gt;run(input_vec, \u0026amp;output_vec); constexpr int k = 3; float max[k] = {0}; int max_index[k] = {0}; // Find the top-k result manually if (output_vec.capacity() \u0026gt; 0) { for (auto output : output_vec) { for (auto i = 0; i \u0026lt; output-\u0026gt;size(); ++i) { for (auto j = 0; j \u0026lt; k; ++j) { if (output-\u0026gt;template data\u0026lt;float\u0026gt;()[i] \u0026gt; max[j]) { for (auto _j = k - 1; _j \u0026gt; j; --_j) { max[_j - 1] = max[_j]; max_index[_j - 1] = max_index[_j]; } max[j] = output-\u0026gt;template data\u0026lt;float\u0026gt;()[i]; max_index[j] = i; goto skip; } } skip:; } } } std::ostringstream stringStream; for (auto j = 0; j \u0026lt; k; ++j) { stringStream \u0026lt;\u0026lt; max_index[j] \u0026lt;\u0026lt; \u0026#34;: \u0026#34; \u0026lt;\u0026lt; max[j]*100 \u0026lt;\u0026lt; \u0026#34;%\\n\u0026#34;; } // if (output_vec.capacity() \u0026gt; 0) { // for (auto output: output_vec) { // for (auto i = 0;i\u0026lt;output-\u0026gt;size();++i) { // stringStream \u0026lt;\u0026lt; output-\u0026gt;template data\u0026lt;float\u0026gt;()[i] \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; // } // } // } return env-\u0026gt;NewStringUTF(stringStream.str().c_str()); } 总结 通过上面的Demo可以看出，通过MNIST数据训练出来的模型在实际运行的准确率还是很堪忧的。所以一个算法从实验室数据到实际应用还有很长的路要走，虽然最近应用于各个领域的深度学习模型层出不穷，测试数据也很好看，但是在实际应用过程中还有很多路要走。虽然DeepLearning已经表现出很强大的黑魔法属性，在实际应用过程中还是有很多工作要做，不然只能停留在Demo阶段。以本文的手写数字识别为例，实际过程的准确率与测试集上的准确率相差甚远，这时候就需要进行大量的优化工作。由于学习深度学习没多久，暂时只能根据以往在机器学习上的经验来进行优化，目前能想到的优化方向有：训练集与实际运行环境要一致、准备更多的训练集、深度另外的模型方法。\n","permalink":"https://lyapple2008.github.io/posts/2018-04-29-android%E7%AB%AF%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/","summary":"\u003cp\u003e  对DeepLearning最初的印象是，大量的训练样本+机器学习，也就是说原来传统的机器学习会遇到的问题，不能解决的问题，换成DeepLearning同样解决不了。比如目标识别中因为光照变化，目标被遮挡，目标的几何变化造成的识别率大幅下降，在DeepLearning中同样也不能很好解决。但是不是说DeepLearning就一无事处，最近几年这么热也决不是因为名字取得好。DeepLearning比较明显的优势就是在特征选择上，想想之前做生物特征识别时，各种找特征，还得考虑什么光照不变，旋转不变，抗尺寸变换，抗遮挡，那叫一个累呀。现在可好啦，一个Convolution   Layer，再配Fully Connected Layer，最后来个Softmax，丢一堆带标签的样本进去自动给你找出特征。当然这个只是一个接触DeepLearning不到一个月的小白的肤浅认识，大家听听就好。\u003c/p\u003e","title":"Android端实现手写数字识别"},{"content":"在做音视频相关的开发过程大体如下所示，对于其中的编码/解码，整个流程相对比较固定，使用ffmpeg可以很好的完成这部分的开发。对其中的帧数据处理（包括音频和视频数据）则相对要多样化一些，比如对视频做尺寸变换，进行音频音量均衡，直播中的美颜处理，多路流合成等等，这些都是属于流程中的帧数据处理。今天要介绍FFmpeg中的AVFilter模块进行帧数据处理的开发，AVFilter模块对帧数据处理进行了很好的抽象。AVFilter中的filter graph（滤波器图）概念非常适合帧数据处理中的多级滤波处理，同时对滤波器的接口进行了规定，后期添加一些自定义的滤波器也是很方便。网上关于AVFilter的介绍大多是基于ffmpeg的命令使用，基于代码实现的很少，最近项目中正好要使用到了AVFilter，写个小结，希望对有同样需求的小伙伴有帮助。\n原始音视频\u0026ndash;\u0026gt;解码\u0026ndash;\u0026gt;帧数据处理\u0026ndash;\u0026gt;编码\u0026ndash;\u0026gt;输出音视频\n1. 主要结构体和API介绍 1 2 3 4 5 6 // 对filters系统的整体管理 struct AVFilterGraph { AVFilterContext **filters; unsigned nb_filters; } 1 2 3 4 5 6 7 // 定义filter本身的能力，拥有的pads，回调函数接口定义 struct AVFilter { const char *name; const AVFilterPad *inputs; const AVFilterPad *outputs; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // filter实例，管理filter与外部的联系 struct AVFilterContext { const AVFilter *filter; char *name; AVFilterPad *input_pads; AVFilterLink **inputs; unsigned nb_inputs AVFilterPad *output_pads; AVFilterLink **outputs; unsigned nb_outputs; struct AVFilterGraph *graph; } 1 2 3 4 5 6 7 8 9 10 11 // 定义两个filters之间的联接 struct AVFilterLink { AVFilterContext *src; AVFilterPad *srcpad; AVFilterContext *dst; AVFilterPad *dstpad; struct AVFilterGraph *graph; } 1 2 3 4 5 6 7 8 9 // 定义filter的输入/输出接口 struct AVFilterPad { const char *name; AVFrame *(*get_video_buffer)(AVFilterLink *link, int w, int h); AVFrame *(*get_audio_buffer)(AVFilterLink *link, int nb_samples); int (*filter_frame)(AVFilterLink *link, AVFrame *frame); int (*request_frame)(AVFilterLink *link); } 1 2 3 4 5 6 7 struct AVFilterInOut { char *name; AVFilterContext *filter_ctx; int pad_idx; struct AVFilterInOut *next; } 在AVFilter模块中定义了AVFilter结构，很个AVFilter都是具有独立功能的节点，如scale filter的作用就是进行图像尺寸变换，overlay filter的作用就是进行图像的叠加，这里需要重点提的是两个特别的filter，一个是buffer，一个是buffersink，滤波器buffer代表filter graph中的源头，原始数据就往这个filter节点输入的；而滤波器buffersink代表filter graph中的输出节点，处理完成的数据从这个filter节点输出。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // 获取FFmpeg中定义的filter，调用该方法前需要先调用avfilter_register_all();进行滤波器注册 AVFilter *avfilter_get_by_name(const char *name); // 往源滤波器buffer中输入待处理的数据 int av_buffersrc_add_frame(AVFilterContext *ctx, AVFrame *frame); // 从目的滤波器buffersink中输出处理完的数据 int av_buffersink_get_frame(AVFilterContext *ctx, AVFrame *frame); // 创建一个滤波器图filter graph AVFilterGraph *avfilter_graph_alloc(void); // 创建一个滤波器实例AVFilterContext，并添加到AVFilterGraph中 int avfilter_graph_create_filter(AVFilterContext **filt_ctx, const AVFilter *filt, const char *name, const char *args, void *opaque, AVFilterGraph *graph_ctx); // 连接两个滤波器节点 int avfilter_link(AVFilterContext *src, unsigned srcpad, AVFilterContext *dst, unsigned dstpad); 2. AVFilter主体框架流程 在利用AVFilter进行音视频数据处理前先将在进行的处理流程绘制出来，现在以FFmpeg filter官方文档中的一个例子为例进行说明。\n1 2 3 4 5 [main] input --\u0026gt; split ---------------------\u0026gt; overlay --\u0026gt; output | ^ |[tmp] [flip]| +-----\u0026gt; crop --\u0026gt; vflip -------+ 这个例子的处理流程如上所示，首先使用split滤波器将input流分成两路流（main和tmp），然后分别对两路流进行处理。对于tmp流，先经过crop滤波器进行裁剪处理，再经过flip滤波器进行垂直方向上的翻转操作，输出的结果命名为flip流。再将main流和flip流输入到overlay滤波器进行合成操作。上图的input就是上面提过的buffer源滤波器，output就是上面的提过的buffersink滤波器。上图中每个节点都是一个AVFilterContext，每个连线就是AVFliterLink。所有这些信息都统一由AVFilterGraph来管理。\n3. 实例实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 extern \u0026#34;C\u0026#34; { #include \u0026lt;libavcodec/avcodec.h\u0026gt; #include \u0026lt;libavformat/avformat.h\u0026gt; #include \u0026lt;libavfilter/avfiltergraph.h\u0026gt; #include \u0026lt;libavfilter/buffersink.h\u0026gt; #include \u0026lt;libavfilter/buffersrc.h\u0026gt; #include \u0026lt;libavutil/opt.h\u0026gt; #include \u0026lt;libavutil/imgutils.h\u0026gt; } int main(int argc, char* argv) { int ret = 0; // input yuv FILE* inFile = NULL; const char* inFileName = \u0026#34;sintel_480x272_yuv420p.yuv\u0026#34;; fopen_s(\u0026amp;inFile, inFileName, \u0026#34;rb+\u0026#34;); if (!inFile) { printf(\u0026#34;Fail to open file\\n\u0026#34;); return -1; } int in_width = 480; int in_height = 272; // output yuv FILE* outFile = NULL; const char* outFileName = \u0026#34;out_crop_vfilter.yuv\u0026#34;; fopen_s(\u0026amp;outFile, outFileName, \u0026#34;wb\u0026#34;); if (!outFile) { printf(\u0026#34;Fail to create file for output\\n\u0026#34;); return -1; } avfilter_register_all(); AVFilterGraph* filter_graph = avfilter_graph_alloc(); if (!filter_graph) { printf(\u0026#34;Fail to create filter graph!\\n\u0026#34;); return -1; } // source filter char args[512]; _snprintf_s(args, sizeof(args), \u0026#34;video_size=%dx%d:pix_fmt=%d:time_base=%d/%d:pixel_aspect=%d/%d\u0026#34;, in_width, in_height, AV_PIX_FMT_YUV420P, 1, 25, 1, 1); AVFilter* bufferSrc = avfilter_get_by_name(\u0026#34;buffer\u0026#34;); AVFilterContext* bufferSrc_ctx; ret = avfilter_graph_create_filter(\u0026amp;bufferSrc_ctx, bufferSrc, \u0026#34;in\u0026#34;, args, NULL, filter_graph); if (ret \u0026lt; 0) { printf(\u0026#34;Fail to create filter bufferSrc\\n\u0026#34;); return -1; } // sink filter AVBufferSinkParams *bufferSink_params; AVFilterContext* bufferSink_ctx; AVFilter* bufferSink = avfilter_get_by_name(\u0026#34;buffersink\u0026#34;); enum AVPixelFormat pix_fmts[] = { AV_PIX_FMT_YUV420P, AV_PIX_FMT_NONE }; bufferSink_params = av_buffersink_params_alloc(); bufferSink_params-\u0026gt;pixel_fmts = pix_fmts; ret = avfilter_graph_create_filter(\u0026amp;bufferSink_ctx, bufferSink, \u0026#34;out\u0026#34;, NULL, bufferSink_params, filter_graph); if (ret \u0026lt; 0) { printf(\u0026#34;Fail to create filter sink filter\\n\u0026#34;); return -1; } // split filter AVFilter *splitFilter = avfilter_get_by_name(\u0026#34;split\u0026#34;); AVFilterContext *splitFilter_ctx; ret = avfilter_graph_create_filter(\u0026amp;splitFilter_ctx, splitFilter, \u0026#34;split\u0026#34;, \u0026#34;outputs=2\u0026#34;, NULL, filter_graph); if (ret \u0026lt; 0) { printf(\u0026#34;Fail to create split filter\\n\u0026#34;); return -1; } // crop filter AVFilter *cropFilter = avfilter_get_by_name(\u0026#34;crop\u0026#34;); AVFilterContext *cropFilter_ctx; ret = avfilter_graph_create_filter(\u0026amp;cropFilter_ctx, cropFilter, \u0026#34;crop\u0026#34;, \u0026#34;out_w=iw:out_h=ih/2:x=0:y=0\u0026#34;, NULL, filter_graph); if (ret \u0026lt; 0) { printf(\u0026#34;Fail to create crop filter\\n\u0026#34;); return -1; } // vflip filter AVFilter *vflipFilter = avfilter_get_by_name(\u0026#34;vflip\u0026#34;); AVFilterContext *vflipFilter_ctx; ret = avfilter_graph_create_filter(\u0026amp;vflipFilter_ctx, vflipFilter, \u0026#34;vflip\u0026#34;, NULL, NULL, filter_graph); if (ret \u0026lt; 0) { printf(\u0026#34;Fail to create vflip filter\\n\u0026#34;); return -1; } // overlay filter AVFilter *overlayFilter = avfilter_get_by_name(\u0026#34;overlay\u0026#34;); AVFilterContext *overlayFilter_ctx; ret = avfilter_graph_create_filter(\u0026amp;overlayFilter_ctx, overlayFilter, \u0026#34;overlay\u0026#34;, \u0026#34;y=0:H/2\u0026#34;, NULL, filter_graph); if (ret \u0026lt; 0) { printf(\u0026#34;Fail to create overlay filter\\n\u0026#34;); return -1; } // src filter to split filter ret = avfilter_link(bufferSrc_ctx, 0, splitFilter_ctx, 0); if (ret != 0) { printf(\u0026#34;Fail to link src filter and split filter\\n\u0026#34;); return -1; } // split filter\u0026#39;s first pad to overlay filter\u0026#39;s main pad ret = avfilter_link(splitFilter_ctx, 0, overlayFilter_ctx, 0); if (ret != 0) { printf(\u0026#34;Fail to link split filter and overlay filter main pad\\n\u0026#34;); return -1; } // split filter\u0026#39;s second pad to crop filter ret = avfilter_link(splitFilter_ctx, 1, cropFilter_ctx, 0); if (ret != 0) { printf(\u0026#34;Fail to link split filter\u0026#39;s second pad and crop filter\\n\u0026#34;); return -1; } // crop filter to vflip filter ret = avfilter_link(cropFilter_ctx, 0, vflipFilter_ctx, 0); if (ret != 0) { printf(\u0026#34;Fail to link crop filter and vflip filter\\n\u0026#34;); return -1; } // vflip filter to overlay filter\u0026#39;s second pad ret = avfilter_link(vflipFilter_ctx, 0, overlayFilter_ctx, 1); if (ret != 0) { printf(\u0026#34;Fail to link vflip filter and overlay filter\u0026#39;s second pad\\n\u0026#34;); return -1; } // overlay filter to sink filter ret = avfilter_link(overlayFilter_ctx, 0, bufferSink_ctx, 0); if (ret != 0) { printf(\u0026#34;Fail to link overlay filter and sink filter\\n\u0026#34;); return -1; } // check filter graph ret = avfilter_graph_config(filter_graph, NULL); if (ret \u0026lt; 0) { printf(\u0026#34;Fail in filter graph\\n\u0026#34;); return -1; } char *graph_str = avfilter_graph_dump(filter_graph, NULL); FILE* graphFile = NULL; fopen_s(\u0026amp;graphFile, \u0026#34;graphFile.txt\u0026#34;, \u0026#34;w\u0026#34;); fprintf(graphFile, \u0026#34;%s\u0026#34;, graph_str); av_free(graph_str); AVFrame *frame_in = av_frame_alloc(); unsigned char *frame_buffer_in = (unsigned char *)av_malloc(av_image_get_buffer_size(AV_PIX_FMT_YUV420P, in_width, in_height, 1)); av_image_fill_arrays(frame_in-\u0026gt;data, frame_in-\u0026gt;linesize, frame_buffer_in, AV_PIX_FMT_YUV420P, in_width, in_height, 1); AVFrame *frame_out = av_frame_alloc(); unsigned char *frame_buffer_out = (unsigned char *)av_malloc(av_image_get_buffer_size(AV_PIX_FMT_YUV420P, in_width, in_height, 1)); av_image_fill_arrays(frame_out-\u0026gt;data, frame_out-\u0026gt;linesize, frame_buffer_out, AV_PIX_FMT_YUV420P, in_width, in_height, 1); frame_in-\u0026gt;width = in_width; frame_in-\u0026gt;height = in_height; frame_in-\u0026gt;format = AV_PIX_FMT_YUV420P; while (1) { if (fread(frame_buffer_in, 1, in_width*in_height * 3 / 2, inFile) != in_width*in_height * 3 / 2) { break; } //input Y,U,V frame_in-\u0026gt;data[0] = frame_buffer_in; frame_in-\u0026gt;data[1] = frame_buffer_in + in_width*in_height; frame_in-\u0026gt;data[2] = frame_buffer_in + in_width*in_height * 5 / 4; if (av_buffersrc_add_frame(bufferSrc_ctx, frame_in) \u0026lt; 0) { printf(\u0026#34;Error while add frame.\\n\u0026#34;); break; } /* pull filtered pictures from the filtergraph */ ret = av_buffersink_get_frame(bufferSink_ctx, frame_out); if (ret \u0026lt; 0) break; //output Y,U,V if (frame_out-\u0026gt;format == AV_PIX_FMT_YUV420P) { for (int i = 0; i \u0026lt; frame_out-\u0026gt;height; i++) { fwrite(frame_out-\u0026gt;data[0] + frame_out-\u0026gt;linesize[0] * i, 1, frame_out-\u0026gt;width, outFile); } for (int i = 0; i \u0026lt; frame_out-\u0026gt;height / 2; i++) { fwrite(frame_out-\u0026gt;data[1] + frame_out-\u0026gt;linesize[1] * i, 1, frame_out-\u0026gt;width / 2, outFile); } for (int i = 0; i \u0026lt; frame_out-\u0026gt;height / 2; i++) { fwrite(frame_out-\u0026gt;data[2] + frame_out-\u0026gt;linesize[2] * i, 1, frame_out-\u0026gt;width / 2, outFile); } } printf(\u0026#34;Process 1 frame!\\n\u0026#34;); av_frame_unref(frame_out); } fclose(inFile); fclose(outFile); av_frame_free(\u0026amp;frame_in); av_frame_free(\u0026amp;frame_out); avfilter_graph_free(\u0026amp;filter_graph); return 0; } github代码仓库\n4. 建立定义Filter滤波器 这部分暂时还没有实践，参考ffmpeg源码中已有的filter和ffmpeg源码中的文档writing_filter.txt，应该实践起来也来难，等后面有时间再补上这部分介绍。\n","permalink":"https://lyapple2008.github.io/posts/2018-3-24-ffmpeg%E4%B8%ADavfilter%E6%A8%A1%E5%9D%97%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97/","summary":"\u003cp\u003e在做音视频相关的开发过程大体如下所示，对于其中的编码/解码，整个流程相对比较固定，使用ffmpeg可以很好的完成这部分的开发。对其中的帧数据处理（包括音频和视频数据）则相对要多样化一些，比如对视频做尺寸变换，进行音频音量均衡，直播中的美颜处理，多路流合成等等，这些都是属于流程中的帧数据处理。今天要介绍FFmpeg中的AVFilter模块进行帧数据处理的开发，AVFilter模块对帧数据处理进行了很好的抽象。AVFilter中的filter graph（滤波器图）概念非常适合帧数据处理中的多级滤波处理，同时对滤波器的接口进行了规定，后期添加一些自定义的滤波器也是很方便。网上关于AVFilter的介绍大多是基于ffmpeg的命令使用，基于代码实现的很少，最近项目中正好要使用到了AVFilter，写个小结，希望对有同样需求的小伙伴有帮助。\u003c/p\u003e","title":"FFmpeg中AVFilter模块实践指南"},{"content":"大年初四立个flag，明年来看看能完成多少，看看脸会不会被打肿。\n1. 健身 体重：55Kg 跑步里程：400Km\n2. 小爱好 Ukelele弹： 《活着》《带你去旅行》\n阅读：5篇读书总结\n3. 技术 短视频应用开发 深度学习\n4. 写博客 至少每月一篇博客\n","permalink":"https://lyapple2008.github.io/posts/2018-2-19-2018%E5%B0%8F%E7%9B%AE%E6%A0%87/","summary":"\u003cp\u003e大年初四立个flag，明年来看看能完成多少，看看脸会不会被打肿。\u003c/p\u003e","title":"2018小目标"},{"content":"1. JNI的作用 JNI（Java Native Interface）提供了一种java与C/C++ 代码进行交互的方式，可以通过java方法调用C/C++ 的实现，和已有的C/C++ 库。\n2. JNI使用方法 首先在java类中声明native方法，与一般的函数声明类似，只是多了个native关键字 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package com.hello.jni; public class HelloJNI { static { System.loadLibrary(\u0026#34;hello\u0026#34;); // Load native library at runtime // hello.dll (Windows) or libhello.so (Unixes) } // Declare a native method sayHello() that receives nothing and returns void private native void sayHello(); // Test Driver public static void main(String[] args) { new HelloJNI().sayHello(); // invoke the native method } } 使用javah命令生成相应的头文件（到elipse工程的bin目录下，打开命令行，注意不用带class后缀） javah com.hello.jni.HelloJNI 这时在bin目录下会生成对应的头文件com_hello_jni_HelloJNI.h\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 /* DO NOT EDIT THIS FILE - it is machine generated */ #include \u0026lt;jni.h\u0026gt; /* Header for class com_hello_jni_HelloJNI */ #ifndef _Included_com_hello_jni_HelloJNI #define _Included_com_hello_jni_HelloJNI #ifdef __cplusplus extern \u0026#34;C\u0026#34; { #endif /* * Class: com_hello_jni_HelloJNI * Method: sayHello * Signature: ()V */ JNIEXPORT void JNICALL Java_com_hello_jni_HelloJNI_sayHello (JNIEnv *, jobject); #ifdef __cplusplus } #endif #endif 然后就是要去实现之前声明的native方法，这里我们使用Visual Studio新建一个dll工程， 这里需要将之前头文件所依赖的jni.h和jni_md.h所在的jdk目录添加到包含目录中，并将项目类型切换到与平台类型一致，否则会出现这样的不匹配错误 1 2 3 4 5 6 7 #include \u0026#34;com_hello_jni_HelloJNI.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; JNIEXPORT void JNICALL Java_com_hello_jni_HelloJNI_sayHello(JNIEnv *, jobject) { printf(\u0026#34;Hello World\u0026#34;); } 1 2 3 4 5 6 7 Exception in thread \u0026#34;main\u0026#34; java.lang.UnsatisfiedLinkError: D:\\Workspace\\AppProject\\MyProjects\\Debug\\hello.dll: Can\u0026#39;t load IA 32-bit .dll on a AMD 64-bit platform at java.lang.ClassLoader$NativeLibrary.load(Native Method) at java.lang.ClassLoader.loadLibrary0(Unknown Source) at java.lang.ClassLoader.loadLibrary(Unknown Source) at java.lang.Runtime.loadLibrary0(Unknown Source) at java.lang.System.loadLibrary(Unknown Source) at com.hello.jni.HelloJNI.\u0026lt;clinit\u0026gt;(HelloJNI.java:5) 将生成的dll文件的路径添加到环境变量中，这样JAVA类就可以调用dll中的C实现方法了 Hello World\n","permalink":"https://lyapple2008.github.io/posts/2018-2-15-jni%E5%85%A5%E9%97%A8%E4%B9%8Bhelloworld%E7%AF%87/","summary":"\u003ch4 id=\"1-jni的作用\"\u003e1. JNI的作用\u003c/h4\u003e\n\u003cp\u003eJNI（Java Native Interface）提供了一种java与C/C++ 代码进行交互的方式，可以通过java方法调用C/C++ 的实现，和已有的C/C++ 库。\u003c/p\u003e","title":"JNI入门之HelloWorld篇"}]